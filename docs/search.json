[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Michael",
    "section": "",
    "text": "GUID: xx133\nEmail: xx133@georgetown.edu\nPhone: 571-220-3128"
  },
  {
    "objectID": "about.html#contact-info",
    "href": "about.html#contact-info",
    "title": "About Michael",
    "section": "",
    "text": "GUID: xx133\nEmail: xx133@georgetown.edu\nPhone: 571-220-3128"
  },
  {
    "objectID": "about.html#background",
    "href": "about.html#background",
    "title": "About Michael",
    "section": "Background",
    "text": "Background\nMy academic journey has been defined by a profound fascination with time series analysis, machine learning, and natural language processing (NLP). During my previous professional experience, I honed my skills in handling large time series datasets, applying a variety of models to uncover underlying trends and patterns. This hands-on work provided a strong foundation in understanding and analyzing temporal data, fueling my passion for predictive modeling. Building on this foundation, I pursued advanced coursework and projects in machine learning, deep learning, and NLP, which significantly enhanced my technical expertise. I have developed proficiency in leveraging PyTorch for deep learning, enabling me to build and fine-tune neural network architectures for diverse applications. Additionally, I have applied cutting-edge NLP techniques to real-world datasets, gaining practical experience in text classification, sentiment analysis, and topic modeling. These academic and project-based experiences have reinforced my belief in the transformative potential of deep learning, particularly in domains such as time series forecasting, healthcare analytics, and text analysis."
  },
  {
    "objectID": "about.html#professional-interest",
    "href": "about.html#professional-interest",
    "title": "About Michael",
    "section": "Professional interest",
    "text": "Professional interest\nMy professional interests encompass the following areas:\n\nExtensive experience in R and Python, with a focus on data analysis, modeling, and application development.\nExpertise in models like ARIMA, SARIMA, and Bayesian Structural Time Series (BSTS), complemented by the integration of deep learning approaches such as LSTMs and Transformer-based architectures for temporal data.\nProficiency in building and training neural networks using PyTorch, with applications in NLP and time series analysis. Projects include implementing advanced NLP techniques such as text embeddings, transformer models, and sequence tagging pipelines.\nSkilled in developing and deploying scalable applications, integrating machine learning models to create robust, data-driven solutions.\n\n\nEducation\n\nMaster of Science(MS) in Data Science and Analytics, Georgetown University(2025)\nBachelor of Science (BS) in Mathematics, University of Massachusetts Amherst (2021)"
  },
  {
    "objectID": "NLP.html",
    "href": "NLP.html",
    "title": "Natual Language Processing",
    "section": "",
    "text": "After conducting EDA on our Diabetes Reddit comments/submissions, the next step is to conduct natural language processing (NLP) on our datasets. Here is a data analysis workflow diagram.\n\n\n\n(Workflow Diagram\n\n\nWhere we input our text data into an NLP preprocessing pipeline, the pipeline processes text data by cleaning and standardizing both comments and submissions. For submissions, it first merges the title and self-text into a unified text field, while comments are processed directly from their corresponding body content. The text then goes through a sequential cleaning process that converts it into a structured document format, breaks it down into individual words, standardizes the text by removing special characters and converting to lowercase, and finally reduces words to their root form through lemmatization. Then we employ the term frequency-inverse document frequency (TF-IDF) on the cleaned dataset to extract words with higher TF-IDF scores, and then use these insights to create dummy variables for sentiment modeling. The sentiment modeling involves using a pre-trained sentiment model: sentimentdl_use_twitter, and we also developed the sentiment model pipeline. Then we extract the sentiment results and create visualizations and data tables to answer our research questions in EDA."
  },
  {
    "objectID": "NLP.html#tf-idf-results",
    "href": "NLP.html#tf-idf-results",
    "title": "Natual Language Processing",
    "section": "TF-IDF Results",
    "text": "TF-IDF Results\nhere are the outputting wordclouds to represent high TF-IDF scores words for both comments and submissions.\n\n\n\nTF-IDF Results for Comments\n\n\n\n\n\nTF-IDF Results for Submissions\n\n\nAs we can see from the wordclouds for both comments and submissions, words like insulin, eat, sugar, diabetes, carb, and blood appear to be the words with high TF-IDF scores. In general, when a word has a high TF-IDF score, it means that the word appears frequently in specific documents but is relatively rare across the entire dataset, suggesting it is a distinctive and meaningful term for those particular discussions. Therefore, these words would be crucial, especially when answering the research questions. From the wordcloud, we can see that people in this subreddit mainly focus on “medication” related subjects and “lifestyle” related subjects.\nIn our dummy variables creation process, for medications, we track various drug classes including insulins, metformin, GLP-1 receptor agonists (like semaglutide), DPP-4 inhibitors, SGLT2 inhibitors, sulfonylureas, thiazolidinediones, and alpha-glucosidase inhibitors. For lifestyle factors, we track diet and exercise, along with common diabetes symptoms including fatigue, thirst, blurred vision, and frequent urination. Technology-related discussions are captured through mentions of glucose monitors, insulin pumps, smartwatches, and diabetes management apps. A detailed dummy variables table is provided below. A detailed dummy variables table is provided below.\n\n\n\n\n\n\n\n\nCategory\nSubcategory\nComponents\n\n\n\n\nMedications\nInsulin\ninsulin, humalog, lantus, novolog, levemir, tresiba, toujeo, basaglar, admelog, apidra, fiasp\n\n\n\nMetformin\nmetformin, glucophage, fortamet, glumetza, riomet, glycon\n\n\n\nGLP-1\nozempic, wegovy, semaglutide, rybelsus\n\n\n\nOther GLP-1\ntrulicity, dulaglutide, victoza, liraglutide, byetta, exenatide, bydureon, mounjaro, tirzepatide\n\n\n\nDPP-4 Inhibitors\njanuvia, sitagliptin, tradjenta, linagliptin, nesina, alogliptin, onglyza, saxagliptin, galvus, vildagliptin\n\n\n\nSGLT2 Inhibitors\nfarxiga, dapagliflozin, jardiance, empagliflozin, invokana, canagliflozin, steglatro, ertugliflozin\n\n\n\nSulfonylureas\nglipizide, glucotrol, glimepiride, amaryl, glyburide, diabeta, glynase, micronase, glibenclamide\n\n\n\nTZDs\nactos, pioglitazone, avandia, rosiglitazone\n\n\n\nAlpha-glucosidase\nprecose, acarbose, glyset, miglitol\n\n\nLifestyle\nDiet\ndiet, nutrition, healthy eating, keto, ketogenic\n\n\n\nExercise\nexercise, gym, fitness, workout, training, weightlifting, walking, steps\n\n\nSymptoms\nPhysical\nfatigue, tiredness, exhaustion\n\n\n\nPhysiological\nthirst, dehydration\n\n\n\nVision\nblurred vision, blurry eyesight\n\n\n\nUrinary\nfrequent urination, peeing often\n\n\nDiabetes Types\nType 1\ntype 1, type-1, t1d\n\n\n\nType 2\ntype 2, type-2, t2d\n\n\n\nGestational\ngestational diabetes, pregnancy diabetes\n\n\nTechnology\nMonitoring\nglucose monitor, CGM, continuous glucose monitor\n\n\n\nInsulin Delivery\npump, insulin pump\n\n\n\nWearables\nsmartwatch, wearable\n\n\n\nSoftware\ndiabetes app, blood sugar tracker, mobile app, software"
  },
  {
    "objectID": "NLP.html#sentiment-model",
    "href": "NLP.html#sentiment-model",
    "title": "Natual Language Processing",
    "section": "Sentiment Model",
    "text": "Sentiment Model\nAfter the dummy variable creation, we processed the transformed datasets through a pre-trained sentiment model. The model, sentimentdl_use_twitter, which is trained with social media text data, is perhaps the best-suited model for our Reddit text data. After running the model, we extracted the sentiment model results and aggregated associated datasets to answer our research questions.\n\nResearch Question 1: Comparison of Sentiments towards Medication and Lifestyle Changes\nDuring the TF-IDF process, key words such as insulin and eat emerged, demonstrating two primary categories: medication and lifestyle factors. These categories represent the most effective treatments for diabetes and align with our interest in investigating sentiment differences towards these approaches. We aggregated the sentiments for both the medication category and lifestyle factors (detailed in the dummy variables creation table). The aggregated sentiments are visualized using stacked bar charts, where blue indicates positive sentiments, yellow represents neutral sentiment, and red denotes negative sentiment.\nThe charts reveal that participants in the diabetes subreddit generally express positive attitudes towards lifestyle factors like changing diets to Keto diets, low carb, high protein diets, start workingout like walking running and lifting weights, with positive sentiments exceeding 50%. In contrast, discussions about medication show predominantly negative sentiments, with positive sentiments falling below 30%. This suggests a general preference for lifestyle modifications over medication-based treatments within the community. While this preference for lifestyle changes might be appropriate for pre-diabetes patients, it raises concerns about medication adherence among diagnosed diabetes patients. For these individuals, proper medication is crucial in preventing severe complications such as kidney failure and retinopathy, from our results, it seems that patients are trying to avoid medication at all costs.(Contributors 2024)\n\n\n\nComparison of Sentiments towards Medication and Lifestyle Changes\n\n\n\n\nResearch Question 2: Comparison of Sentiments towards Different Kinds of Medication\n\n\n\nComparison of Sentiments towards Different Kinds of Medication\n\n\nFor this research question, we examined 9 different classes of diabetes medications: Alpha inhibitors, DPP-4 inhibitors, insulin, metformin, other GLP-1 agonists, semaglutide, SGLT2 inhibitors, sulfonylureas, and TZDs. This represents a comprehensive list of modern diabetes medications, with detailed classifications provided in the table above. The stacked bar charts reveal that while people generally express negative sentiments towards medications, alpha inhibitors stand as an exception with over 60% positive sentiments on the subreddit, while semaglutide emerges as the least favored medication.\nThis finding is both interesting and counter-intuitive, as alpha inhibitors are not typically the first-line treatment for diabetes, nor do they directly influence insulin production in the body (Vijan and Hayward 2002). The notably positive sentiment toward alpha inhibitors could be attributed to two factors: potential class imbalances in the data, or their unique mechanism of action. Since alpha inhibitors don’t interact directly with insulin, patients might prefer them over first-line medications due to concerns about potential side effects, viewing alpha inhibitors as a gentler approach to glucose management. These insights could be valuable for healthcare specialists in understanding patient perspectives on medications and developing more effective educational strategies about the safety and necessity of various diabetes treatments.\n\n\nResearch Question 3: Comparison of Sentiments towards Diabetes and Related Symptoms\nFor this research question, we analyzed how Reddit users in the diabetes subreddit perceive different types of diabetes and their symptoms. We compared the four most common symptoms of diabetes: fatigue, thirst, blurred vision, and frequent urination, alongside three major diabetes categories: gestational diabetes, type 1 diabetes, and type 2 diabetes. The results reveal that users express the most negative sentiments towards blurred vision and gestational diabetes. These insights could inform approaches to improving quality of life for diabetes patients, such as developing targeted symptom-relief medications or educating the public about preventing the progression of diabetes to these more concerning symptoms.\n\n\n\nComparison of Sentiments towards Diabetes and Related Symptoms\n\n\n\n\nResearch Question 4: Overall Sentiments Trend\n\n\n\nOverall Sentiments Trend\n\n\nFor this research question, we investigated the overall sentiment trend between June 2023 and July 2024. We aggregated the sentiment results and excluded neutral sentiments, as they were minimal compared to positive and negative sentiments. From the time series plot, we observed that positive sentiments showed a sharp increase in December 2023. This interesting discovery led us to deeper investigation, where we found that researchers at Case Western Reserve University had made a groundbreaking discovery. They identified an enzyme called SNO-CoA-assisted nitrosylase that blocks insulin production in the body—a discovery that could provide a new target for diabetes treatment (University 2024). While this may not be the sole cause for this sudden increase in positive sentiment, many other factors could have contributed to this trend.\n\n\nResearch Question 5: Comparison of Sentiments towards Technologies\n\n\n\nComparison of Sentiments towards Technologies\n\n\nFor this research question, we investigated sentiments regarding four popular diabetes management technologies: mobile applications, glucose monitors, insulin pumps, and smartwatches. The results were unexpected, with smartwatches receiving the highest positive sentiment despite their limited functionality for diabetes management. More essential devices like insulin pumps and glucose monitors received notably lower positive sentiments, despite their critical role in diabetes care. These findings suggest opportunities for medical device companies to improve user experience design. Both insulin pumps and glucose monitors, while medically necessary, can be complex to use and involve procedures for blood sampling. Medical device manufacturers could focus on designing equipment that minimizes discomfort during use while maintaining clinical effectiveness."
  },
  {
    "objectID": "Introduction.html#introduction",
    "href": "Introduction.html#introduction",
    "title": "Introduction",
    "section": "Introduction",
    "text": "Introduction\nThis project focuses on understanding and addressing two critical chronic conditions: diabetes and cardiovascular diseases (CVD), both of which significantly impact public health worldwide. By leveraging cutting-edge machine learning and natural language processing (NLP) techniques, this study aims to uncover meaningful patterns, classify risks, and identify factors contributing to these diseases. The analysis combines insights from structured datasets, such as the Chronic Disease Indicators (CDI), the Framingham Heart Study, and the Pima Indian Diabetes dataset, along with unstructured data from online communities like the r/diabetes subreddit. This integration provides a holistic view of chronic disease modeling, blending clinical data with real-world patient perspectives.\nMachine learning algorithms are central to this study, with the Naive Bayes classifier showcasing its reliability and accuracy in classifying cardiovascular disease risks, achieving an AUC of 0.97 and highlighting its potential for medical diagnostics. Clustering methods, including K-Means, DBSCAN, and Hierarchical Clustering, were evaluated to group patients into meaningful clusters, aiding in the identification of distinct patient profiles. Moreover, Decision Trees and Random Forests were utilized to provide predictive insights into disease risks and glucose levels, balancing complexity and interpretability to enhance real-world applicability.\nIn addition to structured data, NLP techniques were applied to analyze text datasets from the r/diabetes subreddit, uncovering key themes such as lifestyle changes, medication adherence, and sentiment trends over time. Discussions around lifestyle modifications like diet and exercise were predominantly positive, while sentiments regarding medications, such as GLP-1 agonists, varied due to concerns about side effects and costs. A notable spike in positive sentiment in late 2023 was linked to groundbreaking research developments, highlighting the evolving discourse within the diabetes community.\nBy integrating structured machine learning approaches and unstructured NLP analysis, this project underscores the transformative role of data science in healthcare. It not only provides deeper insights into the risks and management of diabetes and CVD but also emphasizes the value of patient perspectives in shaping medical research. Ultimately, this work contributes to advancing personalized healthcare, improving disease prevention, and fostering a better understanding of chronic disease dynamics."
  },
  {
    "objectID": "Introduction.html#literature-review",
    "href": "Introduction.html#literature-review",
    "title": "Introduction",
    "section": "Literature review",
    "text": "Literature review\n\nEnvironmental/lifestyle factors in the pathogenesis and prevention of type 2 diabetes\n\nThis article talks about different factors related to the causes and prevention of type 2 diabetes (T2D) based on epidemiological studies and intervention trials. This article discusses different parts of lifestyle factors including dietary habits, sleeping disorders, mental affects, body fat mass, etc. This topic is crucial for investigating potential lifestyle risk factors to develop T2D, that could not only improve public awareness but also give data collecting/analysis hypothesis for biostatistician.\nThis article also address current data on personal, environmental, and lifestyle aspects are often limited. A host of environmental or lifestyle-dependent T2D risk factors have been described in prospective epidemiological studies, ranging from energy-dense food consumption to long-term exposure to high levels of fine dust. In these studies, the amount of personal, environmental or lifestyle aspects data that can or has been documented is limited, leaving room for confounding. Kolb and Martin (2017)\nThis article will give me the initial hypothesis of possible factors that contribute do the developmnent of T2D. I will be able explore relevant datasets according to those hypothesis.\n\n\n\nCardiovascular health and potential cardiovascular risk factors in young athletes\n\nThis research paper “Cardiovascular health and potential cardiovascular risk factors in young athletes” investigates the cardiovascular risk in young athletes. Even with regular exercise, a high rate of potential cardiovascular risk factors was found. Elevated systolic blood pressure was observed in 12.6% of participants, double the expected rate for the normal population. Structural vascular and cardiac changes, such as increased pulse wave velocity (PWV) and left ventricular mass (LVM), were also observed. The study found a correlation between higher PWV and systolic blood pressure, which was closely related to elevated hemoglobin levels. This suggests a link between training-induced raised hemoglobin levels and altered vascular properties. Increased LVM was associated with lower resting heart rate, higher metabolic equivalent hours, sports with high dynamic components, and higher systolic BP. The study highlights the importance of regular medical examinations in young athletes to monitor for cardiovascular risk factors​​. Grabitz et al. (2023)"
  },
  {
    "objectID": "Introduction.html#questions-to-answer",
    "href": "Introduction.html#questions-to-answer",
    "title": "Introduction",
    "section": "Questions to answer:",
    "text": "Questions to answer:\n\nWhat lifestyle, dietary, environmental, and genetic factors contribute significantly to the development of Type 2 Diabetes (T2D), and how can these factors be identified through machine learning and NLP techniques?\nHow effective is a Naive Bayes classifier in predicting or classifying outcomes in chronic diseases, particularly cardiovascular diseases?\nHow do different clustering methods, such as K-Means, DBSCAN, and Hierarchical Clustering, perform in grouping patient profiles for chronic disease analysis?\nWhat role do dimensionality reduction techniques, such as PCA (Principal Component Analysis) and t-SNE (t-Distributed Stochastic Neighbor Embedding), play in enhancing the interpretability of complex datasets?\nWhich predictive models, including Decision Trees, Random Forests, and Naive Bayes, offer the highest accuracy in assessing an individual’s risk for Type 2 Diabetes and cardiovascular diseases?\nCan regression models effectively identify early indicators of Type 2 Diabetes, enabling earlier diagnosis and better disease management?\nHow do the performances of Decision Tree models and Random Forest models compare in predicting chronic disease risks, and what factors influence their accuracy?\nWhat insights can be derived from applying natural language processing (NLP) techniques to analyze community discussions from the r/diabetes subreddit?\nWhat are the key risk factors for various cardiovascular diseases, and how do genetic, lifestyle, and demographic influences compare in their impact on chronic disease development?\nHow can data visualization be leveraged to effectively communicate the results and findings of chronic disease research to both scientific and non-technical audiences?"
  },
  {
    "objectID": "Decision_Trees.html",
    "href": "Decision_Trees.html",
    "title": "Decision Tree",
    "section": "",
    "text": "Code\nimport sklearn\nfrom sklearn import datasets\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\nfrom collections import Counter\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_recall_fscore_support"
  },
  {
    "objectID": "Decision_Trees.html#introductions",
    "href": "Decision_Trees.html#introductions",
    "title": "Decision Tree",
    "section": "Introductions",
    "text": "Introductions\n\nDecision Trees:\nA Decision Tree is a tree-like structure where each internal node represents a feature, each branch represents a decision rule, and each leaf node represents the tree outcome. The top node in a tree is known as the root node. It learns to partition on the basis of the attribute value. It partitions the tree in a recursive manner called recursive partitioning. This tree-like structure helps in decision making.\n\n\nRandom Forest:\nRandom Forest is an ensemble learning method, where multiple decision trees are combined to increase the overall performance. While a single decision tree might lead to overfitting, Random Forest averages multiple trees to reduce overfitting and improve prediction accuracy. Furthermore, when splitting each node during the construction of a tree, the best split is found either from all input features or a random subset of size max_features."
  },
  {
    "objectID": "Decision_Trees.html#methods",
    "href": "Decision_Trees.html#methods",
    "title": "Decision Tree",
    "section": "Methods",
    "text": "Methods\nFor this part of the analysis, I will be applying both classification and regression for decision tree analysis, I will be applying the classification methods to binary data Cardiovascular dieases (CVD) and regressino methods to continuous numeric data GLUCOSE.\n\nClassificationRegression\n\n\n\nData and Class Distribution\nFor this part of the analysis, I will be using the Framingham Heart Study dataset@RProjectFramingham for both Decision Tree and Random Forest Analysis, the target variable for this analysis would be \"CVD\", which is Cardio Vascular diseases.\n\n\nCode\n# Load the Frammingham Heart Study data set\ndata = pd.read_csv(\"data/frmgham2.csv\")\ndata.head()\n\n# Split target variables and predictor variables\ny = data['CVD']\nx = data.drop('CVD', axis=1)\nx = x.iloc[:, 1:]\nx = x.values\ny = y.values\n\n\n# Calculating class distribution for 'DIABETES'\ncd = data['CVD'].value_counts(normalize=True) * 100\nprint(cd)\n\n\nCVD\n0    75.066655\n1    24.933345\nName: proportion, dtype: float64\n\n\nThe results show binary data, about roughly 75% of data goes to 0 which is negative test for CVD, and about 25% of data goes to 1 which is positive test for CVD. We have a much higher percentages of data on negative tests, this could lead to a biased model favoring the majority class, we need a more detailed evaluation part to address this issue.\n\n\nEDA\n\n\nCode\ndata_eda = data.iloc[:, 1:]\nprint(data_eda.describe())\n\n\n                SEX       TOTCHOL           AGE         SYSBP         DIABP  \\\ncount  11627.000000  11218.000000  11627.000000  11627.000000  11627.000000   \nmean       1.568074    241.162418     54.792810    136.324116     83.037757   \nstd        0.495366     45.368030      9.564299     22.798625     11.660144   \nmin        1.000000    107.000000     32.000000     83.500000     30.000000   \n25%        1.000000    210.000000     48.000000    120.000000     75.000000   \n50%        2.000000    238.000000     54.000000    132.000000     82.000000   \n75%        2.000000    268.000000     62.000000    149.000000     90.000000   \nmax        2.000000    696.000000     81.000000    295.000000    150.000000   \n\n           CURSMOKE       CIGPDAY           BMI      DIABETES        BPMEDS  \\\ncount  11627.000000  11548.000000  11575.000000  11627.000000  11034.000000   \nmean       0.432528      8.250346     25.877349      0.045584      0.085554   \nstd        0.495448     12.186888      4.102640      0.208589      0.279717   \nmin        0.000000      0.000000     14.430000      0.000000      0.000000   \n25%        0.000000      0.000000     23.095000      0.000000      0.000000   \n50%        0.000000      0.000000     25.480000      0.000000      0.000000   \n75%        1.000000     20.000000     28.070000      0.000000      0.000000   \nmax        1.000000     90.000000     56.800000      1.000000      1.000000   \n\n       ...           CVD      HYPERTEN        TIMEAP        TIMEMI  \\\ncount  ...  11627.000000  11627.000000  11627.000000  11627.000000   \nmean   ...      0.249333      0.743270   7241.556893   7593.846736   \nstd    ...      0.432646      0.436848   2477.780010   2136.730285   \nmin    ...      0.000000      0.000000      0.000000      0.000000   \n25%    ...      0.000000      0.000000   6224.000000   7212.000000   \n50%    ...      0.000000      1.000000   8766.000000   8766.000000   \n75%    ...      0.000000      1.000000   8766.000000   8766.000000   \nmax    ...      1.000000      1.000000   8766.000000   8766.000000   \n\n           TIMEMIFC       TIMECHD      TIMESTRK       TIMECVD       TIMEDTH  \\\ncount  11627.000000  11627.000000  11627.000000  11627.000000  11627.000000   \nmean    7543.036725   7008.153608   7660.880021   7166.082996   7854.102950   \nstd     2192.120311   2641.344513   2011.077091   2541.668477   1788.369623   \nmin        0.000000      0.000000      0.000000      0.000000     26.000000   \n25%     7049.500000   5598.500000   7295.000000   6004.000000   7797.500000   \n50%     8766.000000   8766.000000   8766.000000   8766.000000   8766.000000   \n75%     8766.000000   8766.000000   8766.000000   8766.000000   8766.000000   \nmax     8766.000000   8766.000000   8766.000000   8766.000000   8766.000000   \n\n            TIMEHYP  \ncount  11627.000000  \nmean    3598.956395  \nstd     3464.164659  \nmin        0.000000  \n25%        0.000000  \n50%     2429.000000  \n75%     7329.000000  \nmax     8766.000000  \n\n[8 rows x 38 columns]\n\n\n\nCorrelation Matrix Heatmap\n\n\nCode\ncorr = data.corr()\nsns.set_theme(style=\"white\")\nf, ax = plt.subplots(figsize=(11, 9))  # Set up the matplotlib figure\ncmap = sns.diverging_palette(230, 20, as_cmap=True)     # Generate a custom diverging colormap\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr,  cmap=cmap, vmin=-1, vmax=1, center=0,\n        square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show();\n\n\n\n\n\n\n\n\n\n\n\n\nBaseline model for comparison\n\n\nCode\n## RANDOM CLASSIFIER \ndef random_classifier(y_data):\n    ypred=[];\n    max_label=np.max(y_data); #print(max_label)\n    for i in range(0,len(y_data)):\n        ypred.append(int(np.floor((max_label+1)*np.random.uniform(0,1))))\n\n    print(\"-----RANDOM CLASSIFIER-----\")\n    print(\"count of prediction:\",Counter(ypred).values()) # counts the elements' frequency\n    print(\"probability of prediction:\",np.fromiter(Counter(ypred).values(), dtype=float)/len(y_data)) # counts the elements' frequency\n    print(\"accuracy\",accuracy_score(y_data, ypred))\n    print(\"percision, recall, fscore,\",precision_recall_fscore_support(y_data, ypred))\n\nnp.random.seed(42)    \nrandom.seed(42)\nrandom_classifier(y)\n\n\n-----RANDOM CLASSIFIER-----\ncount of prediction: dict_values([5874, 5753])\nprobability of prediction: [0.50520341 0.49479659]\naccuracy 0.5015051173991572\npercision, recall, fscore, (array([0.7495744 , 0.24821832]), array([0.50446838, 0.49258365]), array([0.60306807, 0.33009709]), array([8728, 2899], dtype=int64))\n\n\n\nInterpretation\n\nCount of Prediction: It shows the number of times each class label (0 and 1) was predicted. The values [5776, 5851] indicate that out of all predictions, 5874 were classified as class ‘0’ and 5851 as class ‘1’.\nProbability of Prediction: This represents the proportion of each class label in the predictions. The values [0.50520341 0.49479659] suggest that approximately 50% of predictions are class ‘0’ which is negative, and 50% are class ‘1’, whcih is positive. This near-even split indicates that the classifier is randomly assigning class labels with almost equal probability to each class.\nAccuracy: An accuracy of approximately 50% is observed. In the context of a binary classifier, an accuracy close to 50% suggests that the classifier’s performance is only slightly better than random guessing. This is expected, given the nature of the random classifier.\nPrecision: Precision for each class (0.7495744 for class ‘0’, 0.24821832 for class ‘1’) indicates the proportion of correctly predicted positive observations out of all predictions in that class. Higher precision for class ‘0’ suggests that the classifier is more reliable when it predicts an instance as class ‘0’ compared to class ‘1’.\nRecall: Recall (0.49702108 for class ‘0’, 0.50446838 for class ‘1’) is the proportion of actual positives correctly identified. The values are close to 50%, showing the classifier’s limited capability in correctly identifying true cases for each class.\nF-Score: F-Score is the harmonic mean of precision and recall. The values (0.60306807 for class ‘0’, 0.33009709 for class ‘1’) indicate the balance between precision and recall for each class, with class ‘0’ having a better balance compared to class ‘1’.\n\n\nModel Tuning\n\nPartion data to train/test split\n\n\nCode\nfrom sklearn.model_selection import train_test_split\ntest_ratio=0.2\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_ratio, random_state=0)\ny_train=y_train.flatten()\ny_test=y_test.flatten()\n\nprint(\"x_train.shape        :\",x_train.shape)\nprint(\"y_train.shape        :\",y_train.shape)\n\nprint(\"X_test.shape     :\",x_test.shape)\nprint(\"y_test.shape     :\",y_test.shape)\n\n\nx_train.shape        : (9301, 37)\ny_train.shape        : (9301,)\nX_test.shape     : (2326, 37)\ny_test.shape     : (2326,)\n\n\n\n\nHyper-Parameter tuning for Decision Tree Classification ((max_depth))\n\n\nCode\nfrom sklearn.metrics import mean_absolute_percentage_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.tree import DecisionTreeClassifier\n\n\n# HYPER PARAMETER SEARCH FOR OPTIMAL NUMBER OF NEIGHBORS \nhyper_param=[]\ntrain_error=[]\ntest_error=[]\n\n# LOOP OVER HYPER-PARAM\nfor i in range(1,40):\n    # INITIALIZE MODEL \n    model = DecisionTreeClassifier(max_depth=i)\n\n    # TRAIN MODEL \n    model.fit(x_train,y_train)\n\n    # OUTPUT PREDICTIONS FOR TRAINING AND TEST SET \n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n\n    # shift=1+np.min(y_train) #add shift to remove division by zero \n    err1=mean_absolute_error(y_train, yp_train) \n    err2=mean_absolute_error(y_test, yp_test) \n    \n    # err1=100.0*np.mean(np.absolute((yp_train-y_train)/y_train))\n    # err2=100.0*np.mean(np.absolute((yp_test-y_test)/y_test))\n\n    hyper_param.append(i)\n    train_error.append(err1)\n    test_error.append(err2)\n\n    if(i==1 or i%10==0):\n        print(\"hyperparam =\",i)\n        print(\" train error:\",err1)\n        print(\" test error:\" ,err2)\n\n\nhyperparam = 1\n train error: 0.15138157187399204\n test error: 0.15047291487532244\nhyperparam = 10\n train error: 0.0\n test error: 0.0030094582975064487\nhyperparam = 20\n train error: 0.0\n test error: 0.0021496130696474634\nhyperparam = 30\n train error: 0.0\n test error: 0.0030094582975064487\n\n\n\nFor max_depth = 1, both the training and test errors are relatively high, with values around 0.15. This suggests that the decision tree is underfitting the data, the depth of tree needs to higer for better model fitting.\nAs the max_depth increase to 10 and beyond, both the training and test errors decrease significantly. When max_depth = 10, the training error is very low (close to 0), indicating that the model is fitting the training data extremely well. However, the test error remains relatively low, suggesting that the model is generalizing reasonably well to unseen data.\nInterestingly, as max_depth furthur increase to 20 and 30, the training error remains very low (close to 0), but the test error starts to increase slightly. This is a sign of overfitting. The model is capturing the data too well, we don’t want this to happen for generalization purposes.\nOverall, the results show that a max_depth of around 10 appears to be a good choice because it achieves low test error without overfitting the training data.\nConvergence plot\n\n\n\nCode\nplt.plot(hyper_param,train_error ,linewidth=2, color='k')\nplt.plot(hyper_param,test_error ,linewidth=2, color='b')\n\nplt.xlabel(\"Depth of tree (max depth)\")\nplt.ylabel(\"Training (black) and test (blue) MAE (error)\")\n\ni=1\nprint(hyper_param[i],train_error[i],test_error[i])\n\n\n2 0.07386302548113106 0.06835769561478934\n\n\n\n\n\n\n\n\n\n\nThe convergence plot clearly demonstrates that as the max_depth hyperparameter increases, both training and test errors demonstrate a pattern of stabilization. This observation alligns with the assumptions made in the previous section. Specifically, it indicates that beyond a certain depth, the decision tree model ceases to substantially improve its fit to the training data, and we could infer that increasing the max_depth further may lead to overfitting, as the model becomes overly complex and starts fitting noise in the training data, while failing to improve its generalization. Hence, the plot supports the notion that there exists an optimal max_depth value that strikes a balance between model complexity and performance on the test dataset.\n\n\n\nHyper-Parameter tuning for Decision Tree Classification (min_samples_splitint)\n\n\nCode\n# HYPER PARAMETER SEARCH FOR OPTIMAL NUMBER OF NEIGHBORS \nhyper_param=[]\ntrain_error=[]\ntest_error=[]\n\n# LOOP OVER HYPER-PARAM\nfor i in range(2,100):\n    # INITIALIZE MODEL \n    model = DecisionTreeClassifier(min_samples_split=i)\n\n    # TRAIN MODEL \n    model.fit(x_train,y_train)\n\n    # OUTPUT PREDICTIONS FOR TRAINING AND TEST SET \n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n\n    # shift=1+np.min(y_train) #add shift to remove division by zero \n    err1=mean_absolute_error(y_train, yp_train) \n    err2=mean_absolute_error(y_test, yp_test) \n    \n    # err1=100.0*np.mean(np.absolute((yp_train-y_train)/y_train))\n    # err2=100.0*np.mean(np.absolute((yp_test-y_test)/y_test))\n\n    hyper_param.append(i)\n    train_error.append(err1)\n    test_error.append(err2)\n\n    if(i%10==0):\n        print(\"hyperparam =\",i)\n        print(\" train error:\",err1)\n        print(\" test error:\" ,err2)\n\n\nhyperparam = 10\n train error: 0.0005375766046661649\n test error: 0.003869303525365434\nhyperparam = 20\n train error: 0.0006450919255993979\n test error: 0.0030094582975064487\nhyperparam = 30\n train error: 0.0008601225674658639\n test error: 0.0034393809114359416\nhyperparam = 40\n train error: 0.0016127298139984947\n test error: 0.004299226139294927\nhyperparam = 50\n train error: 0.0016127298139984947\n test error: 0.004299226139294927\nhyperparam = 60\n train error: 0.0016127298139984947\n test error: 0.004299226139294927\nhyperparam = 70\n train error: 0.0017202451349317277\n test error: 0.004299226139294927\nhyperparam = 80\n train error: 0.0017202451349317277\n test error: 0.004299226139294927\nhyperparam = 90\n train error: 0.0017202451349317277\n test error: 0.004299226139294927\n\n\n\nThe min_samples_split parameter controls the minimum number of samples required to split an internal node during the construction of the decision tree.\nAs min_samples_split increases from 10 to 20, both the training and test errors generally decrease. This suggests that when internal nodes require a larger number of samples to split.\nBeyond min_samples_split = 20, the training error remains low or slightly increases, indicating that the model still fits the training data well, however, the erros for test data follows a clear wave-like pattern.\nOverall, the results suggest that a min_samples_split value around 30 provides a good balance between model complexity and generalization.\n\nConvergence Plot\n\n\nCode\nplt.plot(hyper_param,train_error ,linewidth=2, color='k')\nplt.plot(hyper_param,test_error ,linewidth=2, color='b')\n\nplt.xlabel(\"Minimum number of points in split (min_samples_split)\")\nplt.ylabel(\"Training (black) and test (blue) MAE (error)\")\n\n\nText(0, 0.5, 'Training (black) and test (blue) MAE (error)')\n\n\n\n\n\n\n\n\n\nThe convergence plot highlights a critical observation: there exists a specific range of min_samples_split values where the model performs well on both the training and test datasets. However, as min_samples_split goes beyond this range, the model’s performance on the test data deteriorates noticeably.\nIn practical terms, this indicates that we should seek a min_samples_split value that does not result in a significant drop in test data performance. The objective is to find the sweet spot where the model maintains good generalization to unseen data without overfitting or underfitting.\n\n\nRe-train with optimal parameters\n\n\nCode\n# INITIALIZE MODEL \nmodel = DecisionTreeClassifier(max_depth=10, min_samples_split=30)\nmodel.fit(x_train,y_train)                     # TRAIN MODEL \n\n\n# OUTPUT PREDICTIONS FOR TRAINING AND TEST SET \nyp_train = model.predict(x_train)\nyp_test = model.predict(x_test)\n\nerr1=mean_absolute_error(y_train, yp_train) \nerr2=mean_absolute_error(y_test, yp_test) \n    \nprint(\" train error:\",err1)\nprint(\" test error:\" ,err2)\n\n\n train error: 0.0008601225674658639\n test error: 0.0034393809114359416\n\n\n\n\nParity Plot\n\nPlotting y_pred vs y_data lets you see how good the fit is\nThe closer to the line y=x the better the fit (ypred=ydata –&gt; prefect fit)\n\n\n\nCode\nplt.plot(y_train,yp_train ,\"o\", color='k')\nplt.plot(y_test,yp_test ,\"o\", color='b')\nplt.plot(y_train,y_train ,\"-\", color='r')\n\nplt.xlabel(\"y_data\")\nplt.ylabel(\"y_pred (blue=test)(black=Train)\")\n\n\nText(0, 0.5, 'y_pred (blue=test)(black=Train)')\n\n\n\n\n\n\n\n\n\n\nTraining Data Fit: The black dots are closer to the red line, indicating a good fit for the training data.\nTest Data Fit: The blue dots are spread out, indicating more variance in the fit for the test data compared to the training data.\nOverall Performance: Since some blue points are far from the red line, it suggests the model may not generalize well to new, unseen data, possibly overfitting to the training data.\n\n\n\nPlot Tree\n\n\nCode\nfrom sklearn import tree\ndef plot_tree(model):\n    fig = plt.figure(figsize=(25,20))\n    _ = tree.plot_tree(model, \n                    filled=True)\n    plt.show()\n\nplot_tree(model)\n\n\n\n\n\n\n\n\n\n\n\n\nConclusions(The final reaults are discussed with each section)\n\nThe decision tree with classification analysis gave us insights into heart disease risks, which performed quite well when looking back at the data we trained it on. However, when faced with new, unseen data, the model’s predictions were inconsistent. This suggests that while our model has learned well from past data, it needs to be better tuned to handle new information effectively.\nTo improve, we aim to adjust our model’s complexity. When the model is too simple, and we miss important patterns, when the model is too complex, and we face the risk if overfitting instead of generalization. The goal is to build a model that not only learns from the past but is also ptovide usefull insight for generalization.\n\n\n\n\n\nData understanding\nFor this part of the analysis, I will be using the Framingham Heart Study dataset@RProjectFramingham for both Decision Tree and Random Forest Analysis, the target variable for this analysis would be GLUCOSE, in mmol/L (millimoles per liter).\n\n\nCode\nfrom sklearn.metrics import mean_absolute_percentage_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Load the Frammingham Heart Study data set\ndata = pd.read_csv(\"data/frmgham2.csv\")\nprint(data.head())\ndata.dropna(inplace=True)\n\n# Split target variables and predictor variables\ny = data['GLUCOSE']\nx = data.drop('GLUCOSE', axis=1)\nx = x.iloc[:, 1:]\nx = x.values\ny = y.values\n\n# Normalize \nx=0.1+(x-np.min(x,axis=0))/(np.max(x,axis=0)-np.min(x,axis=0))\ny=0.1+(y-np.min(y,axis=0))/(np.max(y,axis=0)-np.min(y,axis=0))\n\n\n   RANDID  SEX  TOTCHOL  AGE  SYSBP  DIABP  CURSMOKE  CIGPDAY    BMI  \\\n0    2448    1    195.0   39  106.0   70.0         0      0.0  26.97   \n1    2448    1    209.0   52  121.0   66.0         0      0.0    NaN   \n2    6238    2    250.0   46  121.0   81.0         0      0.0  28.73   \n3    6238    2    260.0   52  105.0   69.5         0      0.0  29.43   \n4    6238    2    237.0   58  108.0   66.0         0      0.0  28.50   \n\n   DIABETES  ...  CVD  HYPERTEN  TIMEAP  TIMEMI  TIMEMIFC  TIMECHD  TIMESTRK  \\\n0         0  ...    1         0    8766    6438      6438     6438      8766   \n1         0  ...    1         0    8766    6438      6438     6438      8766   \n2         0  ...    0         0    8766    8766      8766     8766      8766   \n3         0  ...    0         0    8766    8766      8766     8766      8766   \n4         0  ...    0         0    8766    8766      8766     8766      8766   \n\n   TIMECVD  TIMEDTH  TIMEHYP  \n0     6438     8766     8766  \n1     6438     8766     8766  \n2     8766     8766     8766  \n3     8766     8766     8766  \n4     8766     8766     8766  \n\n[5 rows x 39 columns]\n\n\nC:\\Users\\xinzh\\AppData\\Local\\Temp\\ipykernel_35996\\2745761356.py:18: RuntimeWarning:\n\ninvalid value encountered in divide\n\n\n\n\n\nEDA\n\n\nCode\ndata_eda = data.iloc[:, 1:]\nprint(data_eda.describe())\n\n\n               SEX      TOTCHOL          AGE        SYSBP        DIABP  \\\ncount  2236.000000  2236.000000  2236.000000  2236.000000  2236.000000   \nmean      1.565742   237.598837    60.166816   139.027728    80.999106   \nstd       0.495770    45.288008     8.295838    22.343693    11.330523   \nmin       1.000000   112.000000    44.000000    86.000000    30.000000   \n25%       1.000000   206.000000    53.000000   122.000000    73.000000   \n50%       2.000000   235.000000    59.000000   136.000000    80.000000   \n75%       2.000000   265.000000    67.000000   153.000000    88.000000   \nmax       2.000000   625.000000    81.000000   246.000000   130.000000   \n\n          CURSMOKE      CIGPDAY          BMI     DIABETES       BPMEDS  ...  \\\ncount  2236.000000  2236.000000  2236.000000  2236.000000  2236.000000  ...   \nmean      0.348837     6.864490    25.752227     0.070662     0.144454  ...   \nstd       0.476709    11.622125     3.883322     0.256317     0.351629  ...   \nmin       0.000000     0.000000    14.430000     0.000000     0.000000  ...   \n25%       0.000000     0.000000    23.187500     0.000000     0.000000  ...   \n50%       0.000000     0.000000    25.380000     0.000000     0.000000  ...   \n75%       1.000000    10.000000    27.845000     0.000000     0.000000  ...   \nmax       1.000000    80.000000    46.520000     1.000000     1.000000  ...   \n\n               CVD     HYPERTEN       TIMEAP       TIMEMI     TIMEMIFC  \\\ncount  2236.000000  2236.000000  2236.000000  2236.000000  2236.000000   \nmean      0.233900     0.738372  7731.038909  8074.634615  8030.673971   \nstd       0.423404     0.439619  2032.270314  1601.230796  1661.646908   \nmin       0.000000     0.000000     0.000000     0.000000     0.000000   \n25%       0.000000     0.000000  7570.750000  8609.500000  8454.000000   \n50%       0.000000     1.000000  8766.000000  8766.000000  8766.000000   \n75%       0.000000     1.000000  8766.000000  8766.000000  8766.000000   \nmax       1.000000     1.000000  8766.000000  8766.000000  8766.000000   \n\n           TIMECHD     TIMESTRK      TIMECVD      TIMEDTH      TIMEHYP  \ncount  2236.000000  2236.000000  2236.000000  2236.000000  2236.000000  \nmean   7498.554562  8167.498211  7640.489267  8351.642665  3943.845707  \nstd    2255.888945  1342.934286  2138.744071   969.038296  3495.722584  \nmin       0.000000     0.000000     0.000000  4182.000000     0.000000  \n25%    6921.250000  8673.250000  7347.500000  8766.000000     0.000000  \n50%    8766.000000  8766.000000  8766.000000  8766.000000  2973.500000  \n75%    8766.000000  8766.000000  8766.000000  8766.000000  7989.250000  \nmax    8766.000000  8766.000000  8766.000000  8766.000000  8766.000000  \n\n[8 rows x 38 columns]\n\n\n\nCorrelation Matrix Heatmap\n\n\nCode\ncorr = data.corr()\nsns.set_theme(style=\"white\")\nf, ax = plt.subplots(figsize=(11, 9))  # Set up the matplotlib figure\ncmap = sns.diverging_palette(230, 20, as_cmap=True)     # Generate a custom diverging colormap\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr,  cmap=cmap, vmin=-1, vmax=1, center=0,\n        square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show();\n\n\n\n\n\n\n\n\n\n\n\n\nModel Tuning\n\nPartion data to train/test split\n\n\nCode\nfrom sklearn.model_selection import train_test_split\ntest_ratio=0.2\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_ratio, random_state=0)\ny_train=y_train.flatten()\ny_test=y_test.flatten()\n\nprint(\"x_train.shape        :\",x_train.shape)\nprint(\"y_train.shape        :\",y_train.shape)\n\nprint(\"X_test.shape     :\",x_test.shape)\nprint(\"y_test.shape     :\",y_test.shape)\n\n\nx_train.shape        : (1788, 37)\ny_train.shape        : (1788,)\nX_test.shape     : (448, 37)\ny_test.shape     : (448,)\n\n\n\n\nHyper-Parameter tuning for Decision Tree Classification ((max_depth))\n\n\nCode\nfrom sklearn.metrics import mean_absolute_percentage_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.tree import DecisionTreeClassifier\n\n\n# HYPER PARAMETER SEARCH FOR OPTIMAL NUMBER OF NEIGHBORS \nhyper_param=[]\ntrain_error=[]\ntest_error=[]\n\n# LOOP OVER HYPER-PARAM\nfor i in range(1,40):\n    # INITIALIZE MODEL \n    model = DecisionTreeRegressor(max_depth=i)\n\n    # TRAIN MODEL \n    model.fit(x_train,y_train)\n\n    # OUTPUT PREDICTIONS FOR TRAINING AND TEST SET \n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n\n    # shift=1+np.min(y_train) #add shift to remove division by zero \n    err1=mean_absolute_error(y_train, yp_train) \n    err2=mean_absolute_error(y_test, yp_test) \n    \n    # err1=100.0*np.mean(np.absolute((yp_train-y_train)/y_train))\n    # err2=100.0*np.mean(np.absolute((yp_test-y_test)/y_test))\n\n    hyper_param.append(i)\n    train_error.append(err1)\n    test_error.append(err2)\n\n    if(i==1 or i%10==0):\n        print(\"hyperparam =\",i)\n        print(\" train error:\",err1)\n        print(\" test error:\" ,err2)\n\n\nhyperparam = 1\n train error: 0.035268787108589356\n test error: 0.03418032867241032\nhyperparam = 10\n train error: 0.02038622946162721\n test error: 0.04358912209657024\nhyperparam = 20\n train error: 0.005180126792851705\n test error: 0.047937823543169375\nhyperparam = 30\n train error: 0.00022663570019033476\n test error: 0.04882001990611059\n\n\n\nFor max_depth = 1, both the training and test errors are relatively low, with values around 0.03. This suggests that the decision tree is fitting the data well.\nAs the max_depth increase to 10 and beyond, the training erros starts to drop, however, the test error starts to go up.\nInterestingly, as max_depth furthur increase to 20 and 30, the training error remains very low (close to 0), but the test error starts to increase slightly. This is a sign of overfitting. The model is capturing the data too well, we don’t want this to happen for generalization purposes.\nOverall, the results show that a max_depth of around 1 to 10 appears to be a good choice because it achieves low test error without overfitting the training data.\nConvergence plot\n\n\n\nCode\nplt.plot(hyper_param,train_error ,linewidth=2, color='k')\nplt.plot(hyper_param,test_error ,linewidth=2, color='b')\n\nplt.xlabel(\"Depth of tree (max depth)\")\nplt.ylabel(\"Training (black) and test (blue) MAE (error)\")\n\ni=1\nprint(hyper_param[i],train_error[i],test_error[i])\n\n\n2 0.03442784374001432 0.032813904712789006\n\n\n\n\n\n\n\n\n\n\nThe convergence plot clearly demonstrates that as the max_depth hyperparameter increases, The point at which the lines start to diverge significantly is critical; it marks the transition from underfitting to the optimal complexity and then to overfitting. According to the plot a max_depth of 0 to 5 is optimal for a balance between training error and testing error.\n\n\n\nHyper-Parameter tuning for Decision Tree Regression (min_samples_splitint)\n\n\nCode\n# HYPER PARAMETER SEARCH FOR OPTIMAL NUMBER OF NEIGHBORS \nhyper_param=[]\ntrain_error=[]\ntest_error=[]\n\n# LOOP OVER HYPER-PARAM\nfor i in range(2,100):\n    # INITIALIZE MODEL \n    model = DecisionTreeRegressor(min_samples_split=i)\n\n    # TRAIN MODEL \n    model.fit(x_train,y_train)\n\n    # OUTPUT PREDICTIONS FOR TRAINING AND TEST SET \n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n\n    # shift=1+np.min(y_train) #add shift to remove division by zero \n    err1=mean_absolute_error(y_train, yp_train) \n    err2=mean_absolute_error(y_test, yp_test) \n    \n    # err1=100.0*np.mean(np.absolute((yp_train-y_train)/y_train))\n    # err2=100.0*np.mean(np.absolute((yp_test-y_test)/y_test))\n\n    hyper_param.append(i)\n    train_error.append(err1)\n    test_error.append(err2)\n\n    if(i%10==0):\n        print(\"hyperparam =\",i)\n        print(\" train error:\",err1)\n        print(\" test error:\" ,err2)\n\n\nhyperparam = 10\n train error: 0.014984530706046054\n test error: 0.045871667532753846\nhyperparam = 20\n train error: 0.021962415929112074\n test error: 0.043417885579028495\nhyperparam = 30\n train error: 0.026579851814848933\n test error: 0.040156857780947484\nhyperparam = 40\n train error: 0.0279409667047845\n test error: 0.038751571864489465\nhyperparam = 50\n train error: 0.028391822245632067\n test error: 0.038300992283321324\nhyperparam = 60\n train error: 0.029240913243283456\n test error: 0.036936685875858556\nhyperparam = 70\n train error: 0.029627133194660586\n test error: 0.036694713507271985\nhyperparam = 80\n train error: 0.03032461525639734\n test error: 0.03656614931125601\nhyperparam = 90\n train error: 0.03058836291359766\n test error: 0.036432623399717076\n\n\n\nThe min_samples_split parameter controls the minimum number of samples required to split an internal node during the construction of the decision tree.\nAs min_samples_split increases from 10 to 20, both the training and test errors generally decrease. This suggests that when internal nodes require a larger number of samples to split.\nBeyond min_samples_split = 20, the training error remains low, indicating that the model still fits the training data well, the test error drops at the same time.\n\nConvergence Plot\n\n\nCode\nplt.plot(hyper_param,train_error ,linewidth=2, color='k')\nplt.plot(hyper_param,test_error ,linewidth=2, color='b')\n\nplt.xlabel(\"Minimum number of points in split (min_samples_split)\")\nplt.ylabel(\"Training (black) and test (blue) MAE (error)\")\n\n\nText(0, 0.5, 'Training (black) and test (blue) MAE (error)')\n\n\n\n\n\n\n\n\n\nThe convergence plot demonstrates that test error and train error converge to a point, roughly min_samples_spliy = 50.\n\n\nRe-train with optimal parameters\n\n\nCode\n# INITIALIZE MODEL \nmodel = DecisionTreeRegressor(max_depth=1,min_samples_split=50)\nmodel.fit(x_train,y_train)                     # TRAIN MODEL \n\n\n# OUTPUT PREDICTIONS FOR TRAINING AND TEST SET \nyp_train = model.predict(x_train)\nyp_test = model.predict(x_test)\n\nerr1=mean_absolute_error(y_train, yp_train) \nerr2=mean_absolute_error(y_test, yp_test) \n    \nprint(\" train error:\",err1)\nprint(\" test error:\" ,err2)\n\n\n train error: 0.03526878710858939\n test error: 0.03418032867241034\n\n\n\n\nParity Plot\n\nPlotting y_pred vs y_data lets you see how good the fit is\nThe closer to the line y=x the better the fit (ypred=ydata –&gt; prefect fit)\n\n\n\nCode\nplt.plot(y_train,yp_train ,\"o\", color='k')\nplt.plot(y_test,yp_test ,\"o\", color='b')\nplt.plot(y_train,y_train ,\"-\", color='r')\n\nplt.xlabel(\"y_data\")\nplt.ylabel(\"y_pred (blue=test)(black=Train)\")\n\n\nText(0, 0.5, 'y_pred (blue=test)(black=Train)')\n\n\n\n\n\n\n\n\n\n\n\nPlot Tree\n\n\nCode\nfrom sklearn import tree\ndef plot_tree(model):\n    fig = plt.figure(figsize=(25,20))\n    _ = tree.plot_tree(model, \n                    filled=True)\n    plt.show()\n\nplot_tree(model)\n\n\n\n\n\n\n\n\n\n\n\n\nConclusions(The final reaults are discussed with each section)\n\nThe decision tree with regression analysis gave us insights into heart disease risks, which performed quite well when looking back at the data we trained it on. However, when faced with new, unseen data, the model’s predictions were inconsistent. This suggests that while our model has learned well from past data, it needs to be better tuned to handle new information effectively.\nTo improve, we aim to adjust our model’s complexity. When the model is too simple, and we miss important patterns, when the model is too complex, and we face the risk if overfitting instead of generalization. The goal is to build a model that not only learns from the past but is also ptovide usefull insight for generalization."
  },
  {
    "objectID": "Decision_Trees.html#data-and-class-distribution",
    "href": "Decision_Trees.html#data-and-class-distribution",
    "title": "Decision Tree",
    "section": "Data and Class Distribution",
    "text": "Data and Class Distribution\nFor this part of the analysis, I will be using the Framingham Heart Study dataset@RProjectFramingham for both Decision Tree and Random Forest Analysis, the target variable for this analysis would be \"CVD\", which is Cardio Vascular diseases.\n\n\nCode\n# Load the Frammingham Heart Study data set\ndata = pd.read_csv(\"data/frmgham2.csv\")\ndata.head()\n\n# Split target variables and predictor variables\ny = data['CVD']\nx = data.drop('CVD', axis=1)\nx = x.iloc[:, 1:]\nx = x.values\ny = y.values\n\n\n# Calculating class distribution for 'DIABETES'\ncd = data['CVD'].value_counts(normalize=True) * 100\nprint(cd)\n\n\nCVD\n0    75.066655\n1    24.933345\nName: proportion, dtype: float64\n\n\nThe results show binary data, about roughly 75% of data goes to 0 which is negative test for CVD, and about 25% of data goes to 1 which is positive test for CVD. We have a much higher percentages of data on negative tests, this could lead to a biased model favoring the majority class, we need a more detailed evaluation part to address this issue."
  },
  {
    "objectID": "Decision_Trees.html#eda",
    "href": "Decision_Trees.html#eda",
    "title": "Decision Tree",
    "section": "EDA",
    "text": "EDA\n\n\nCode\ndata_eda = data.iloc[:, 1:]\nprint(data_eda.describe())\n\n\n                SEX       TOTCHOL           AGE         SYSBP         DIABP  \\\ncount  11627.000000  11218.000000  11627.000000  11627.000000  11627.000000   \nmean       1.568074    241.162418     54.792810    136.324116     83.037757   \nstd        0.495366     45.368030      9.564299     22.798625     11.660144   \nmin        1.000000    107.000000     32.000000     83.500000     30.000000   \n25%        1.000000    210.000000     48.000000    120.000000     75.000000   \n50%        2.000000    238.000000     54.000000    132.000000     82.000000   \n75%        2.000000    268.000000     62.000000    149.000000     90.000000   \nmax        2.000000    696.000000     81.000000    295.000000    150.000000   \n\n           CURSMOKE       CIGPDAY           BMI      DIABETES        BPMEDS  \\\ncount  11627.000000  11548.000000  11575.000000  11627.000000  11034.000000   \nmean       0.432528      8.250346     25.877349      0.045584      0.085554   \nstd        0.495448     12.186888      4.102640      0.208589      0.279717   \nmin        0.000000      0.000000     14.430000      0.000000      0.000000   \n25%        0.000000      0.000000     23.095000      0.000000      0.000000   \n50%        0.000000      0.000000     25.480000      0.000000      0.000000   \n75%        1.000000     20.000000     28.070000      0.000000      0.000000   \nmax        1.000000     90.000000     56.800000      1.000000      1.000000   \n\n       ...           CVD      HYPERTEN        TIMEAP        TIMEMI  \\\ncount  ...  11627.000000  11627.000000  11627.000000  11627.000000   \nmean   ...      0.249333      0.743270   7241.556893   7593.846736   \nstd    ...      0.432646      0.436848   2477.780010   2136.730285   \nmin    ...      0.000000      0.000000      0.000000      0.000000   \n25%    ...      0.000000      0.000000   6224.000000   7212.000000   \n50%    ...      0.000000      1.000000   8766.000000   8766.000000   \n75%    ...      0.000000      1.000000   8766.000000   8766.000000   \nmax    ...      1.000000      1.000000   8766.000000   8766.000000   \n\n           TIMEMIFC       TIMECHD      TIMESTRK       TIMECVD       TIMEDTH  \\\ncount  11627.000000  11627.000000  11627.000000  11627.000000  11627.000000   \nmean    7543.036725   7008.153608   7660.880021   7166.082996   7854.102950   \nstd     2192.120311   2641.344513   2011.077091   2541.668477   1788.369623   \nmin        0.000000      0.000000      0.000000      0.000000     26.000000   \n25%     7049.500000   5598.500000   7295.000000   6004.000000   7797.500000   \n50%     8766.000000   8766.000000   8766.000000   8766.000000   8766.000000   \n75%     8766.000000   8766.000000   8766.000000   8766.000000   8766.000000   \nmax     8766.000000   8766.000000   8766.000000   8766.000000   8766.000000   \n\n            TIMEHYP  \ncount  11627.000000  \nmean    3598.956395  \nstd     3464.164659  \nmin        0.000000  \n25%        0.000000  \n50%     2429.000000  \n75%     7329.000000  \nmax     8766.000000  \n\n[8 rows x 38 columns]\n\n\n\nCorrelation Matrix Heatmap\n\n\nCode\ncorr = data.corr()\nsns.set_theme(style=\"white\")\nf, ax = plt.subplots(figsize=(11, 9))  # Set up the matplotlib figure\ncmap = sns.diverging_palette(230, 20, as_cmap=True)     # Generate a custom diverging colormap\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr,  cmap=cmap, vmin=-1, vmax=1, center=0,\n        square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show();"
  },
  {
    "objectID": "Decision_Trees.html#baseline-model-for-comparison",
    "href": "Decision_Trees.html#baseline-model-for-comparison",
    "title": "Decision Tree",
    "section": "Baseline model for comparison",
    "text": "Baseline model for comparison\n\n\nCode\n## RANDOM CLASSIFIER \ndef random_classifier(y_data):\n    ypred=[];\n    max_label=np.max(y_data); #print(max_label)\n    for i in range(0,len(y_data)):\n        ypred.append(int(np.floor((max_label+1)*np.random.uniform(0,1))))\n\n    print(\"-----RANDOM CLASSIFIER-----\")\n    print(\"count of prediction:\",Counter(ypred).values()) # counts the elements' frequency\n    print(\"probability of prediction:\",np.fromiter(Counter(ypred).values(), dtype=float)/len(y_data)) # counts the elements' frequency\n    print(\"accuracy\",accuracy_score(y_data, ypred))\n    print(\"percision, recall, fscore,\",precision_recall_fscore_support(y_data, ypred))\n\nnp.random.seed(42)    \nrandom.seed(42)\nrandom_classifier(y)\n\n\n-----RANDOM CLASSIFIER-----\ncount of prediction: dict_values([5874, 5753])\nprobability of prediction: [0.50520341 0.49479659]\naccuracy 0.5015051173991572\npercision, recall, fscore, (array([0.7495744 , 0.24821832]), array([0.50446838, 0.49258365]), array([0.60306807, 0.33009709]), array([8728, 2899], dtype=int64))\n\n\n\nInterpretation\n\nCount of Prediction: It shows the number of times each class label (0 and 1) was predicted. The values [5776, 5851] indicate that out of all predictions, 5874 were classified as class ‘0’ and 5851 as class ‘1’.\nProbability of Prediction: This represents the proportion of each class label in the predictions. The values [0.50520341 0.49479659] suggest that approximately 50% of predictions are class ‘0’ which is negative, and 50% are class ‘1’, whcih is positive. This near-even split indicates that the classifier is randomly assigning class labels with almost equal probability to each class.\nAccuracy: An accuracy of approximately 50% is observed. In the context of a binary classifier, an accuracy close to 50% suggests that the classifier’s performance is only slightly better than random guessing. This is expected, given the nature of the random classifier.\nPrecision: Precision for each class (0.7495744 for class ‘0’, 0.24821832 for class ‘1’) indicates the proportion of correctly predicted positive observations out of all predictions in that class. Higher precision for class ‘0’ suggests that the classifier is more reliable when it predicts an instance as class ‘0’ compared to class ‘1’.\nRecall: Recall (0.49702108 for class ‘0’, 0.50446838 for class ‘1’) is the proportion of actual positives correctly identified. The values are close to 50%, showing the classifier’s limited capability in correctly identifying true cases for each class.\nF-Score: F-Score is the harmonic mean of precision and recall. The values (0.60306807 for class ‘0’, 0.33009709 for class ‘1’) indicate the balance between precision and recall for each class, with class ‘0’ having a better balance compared to class ‘1’."
  },
  {
    "objectID": "Decision_Trees.html#model-tuning",
    "href": "Decision_Trees.html#model-tuning",
    "title": "Decision Tree",
    "section": "Model Tuning",
    "text": "Model Tuning\n\nPartion data to train/test split\n\n\nCode\nfrom sklearn.model_selection import train_test_split\ntest_ratio=0.2\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_ratio, random_state=0)\ny_train=y_train.flatten()\ny_test=y_test.flatten()\n\nprint(\"x_train.shape        :\",x_train.shape)\nprint(\"y_train.shape        :\",y_train.shape)\n\nprint(\"X_test.shape     :\",x_test.shape)\nprint(\"y_test.shape     :\",y_test.shape)\n\n\nx_train.shape        : (9301, 37)\ny_train.shape        : (9301,)\nX_test.shape     : (2326, 37)\ny_test.shape     : (2326,)\n\n\n\n\nHyper-Parameter tuning for Decision Tree Classification ((max_depth))\n\n\nCode\nfrom sklearn.metrics import mean_absolute_percentage_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.tree import DecisionTreeClassifier\n\n\n# HYPER PARAMETER SEARCH FOR OPTIMAL NUMBER OF NEIGHBORS \nhyper_param=[]\ntrain_error=[]\ntest_error=[]\n\n# LOOP OVER HYPER-PARAM\nfor i in range(1,40):\n    # INITIALIZE MODEL \n    model = DecisionTreeClassifier(max_depth=i)\n\n    # TRAIN MODEL \n    model.fit(x_train,y_train)\n\n    # OUTPUT PREDICTIONS FOR TRAINING AND TEST SET \n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n\n    # shift=1+np.min(y_train) #add shift to remove division by zero \n    err1=mean_absolute_error(y_train, yp_train) \n    err2=mean_absolute_error(y_test, yp_test) \n    \n    # err1=100.0*np.mean(np.absolute((yp_train-y_train)/y_train))\n    # err2=100.0*np.mean(np.absolute((yp_test-y_test)/y_test))\n\n    hyper_param.append(i)\n    train_error.append(err1)\n    test_error.append(err2)\n\n    if(i==1 or i%10==0):\n        print(\"hyperparam =\",i)\n        print(\" train error:\",err1)\n        print(\" test error:\" ,err2)\n\n\nhyperparam = 1\n train error: 0.15138157187399204\n test error: 0.15047291487532244\nhyperparam = 10\n train error: 0.0\n test error: 0.0030094582975064487\nhyperparam = 20\n train error: 0.0\n test error: 0.0021496130696474634\nhyperparam = 30\n train error: 0.0\n test error: 0.0030094582975064487\n\n\n\nFor max_depth = 1, both the training and test errors are relatively high, with values around 0.15. This suggests that the decision tree is underfitting the data, the depth of tree needs to higer for better model fitting.\nAs the max_depth increase to 10 and beyond, both the training and test errors decrease significantly. When max_depth = 10, the training error is very low (close to 0), indicating that the model is fitting the training data extremely well. However, the test error remains relatively low, suggesting that the model is generalizing reasonably well to unseen data.\nInterestingly, as max_depth furthur increase to 20 and 30, the training error remains very low (close to 0), but the test error starts to increase slightly. This is a sign of overfitting. The model is capturing the data too well, we don’t want this to happen for generalization purposes.\nOverall, the results show that a max_depth of around 10 appears to be a good choice because it achieves low test error without overfitting the training data.\nConvergence plot\n\n\n\nCode\nplt.plot(hyper_param,train_error ,linewidth=2, color='k')\nplt.plot(hyper_param,test_error ,linewidth=2, color='b')\n\nplt.xlabel(\"Depth of tree (max depth)\")\nplt.ylabel(\"Training (black) and test (blue) MAE (error)\")\n\ni=1\nprint(hyper_param[i],train_error[i],test_error[i])\n\n\n2 0.07386302548113106 0.06835769561478934\n\n\n\n\n\n\n\n\n\n\nThe convergence plot clearly demonstrates that as the max_depth hyperparameter increases, both training and test errors demonstrate a pattern of stabilization. This observation alligns with the assumptions made in the previous section. Specifically, it indicates that beyond a certain depth, the decision tree model ceases to substantially improve its fit to the training data, and we could infer that increasing the max_depth further may lead to overfitting, as the model becomes overly complex and starts fitting noise in the training data, while failing to improve its generalization. Hence, the plot supports the notion that there exists an optimal max_depth value that strikes a balance between model complexity and performance on the test dataset.\n\n\n\nHyper-Parameter tuning for Decision Tree Classification (min_samples_splitint)\n\n\nCode\n# HYPER PARAMETER SEARCH FOR OPTIMAL NUMBER OF NEIGHBORS \nhyper_param=[]\ntrain_error=[]\ntest_error=[]\n\n# LOOP OVER HYPER-PARAM\nfor i in range(2,100):\n    # INITIALIZE MODEL \n    model = DecisionTreeClassifier(min_samples_split=i)\n\n    # TRAIN MODEL \n    model.fit(x_train,y_train)\n\n    # OUTPUT PREDICTIONS FOR TRAINING AND TEST SET \n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n\n    # shift=1+np.min(y_train) #add shift to remove division by zero \n    err1=mean_absolute_error(y_train, yp_train) \n    err2=mean_absolute_error(y_test, yp_test) \n    \n    # err1=100.0*np.mean(np.absolute((yp_train-y_train)/y_train))\n    # err2=100.0*np.mean(np.absolute((yp_test-y_test)/y_test))\n\n    hyper_param.append(i)\n    train_error.append(err1)\n    test_error.append(err2)\n\n    if(i%10==0):\n        print(\"hyperparam =\",i)\n        print(\" train error:\",err1)\n        print(\" test error:\" ,err2)\n\n\nhyperparam = 10\n train error: 0.0005375766046661649\n test error: 0.003869303525365434\nhyperparam = 20\n train error: 0.0006450919255993979\n test error: 0.0030094582975064487\nhyperparam = 30\n train error: 0.0008601225674658639\n test error: 0.0034393809114359416\nhyperparam = 40\n train error: 0.0016127298139984947\n test error: 0.004299226139294927\nhyperparam = 50\n train error: 0.0016127298139984947\n test error: 0.004299226139294927\nhyperparam = 60\n train error: 0.0016127298139984947\n test error: 0.004299226139294927\nhyperparam = 70\n train error: 0.0017202451349317277\n test error: 0.004299226139294927\nhyperparam = 80\n train error: 0.0017202451349317277\n test error: 0.004299226139294927\nhyperparam = 90\n train error: 0.0017202451349317277\n test error: 0.004299226139294927\n\n\n\nThe min_samples_split parameter controls the minimum number of samples required to split an internal node during the construction of the decision tree.\nAs min_samples_split increases from 10 to 20, both the training and test errors generally decrease. This suggests that when internal nodes require a larger number of samples to split.\nBeyond min_samples_split = 20, the training error remains low or slightly increases, indicating that the model still fits the training data well, however, the erros for test data follows a clear wave-like pattern.\nOverall, the results suggest that a min_samples_split value around 30 provides a good balance between model complexity and generalization.\n\nConvergence Plot\n\n\nCode\nplt.plot(hyper_param,train_error ,linewidth=2, color='k')\nplt.plot(hyper_param,test_error ,linewidth=2, color='b')\n\nplt.xlabel(\"Minimum number of points in split (min_samples_split)\")\nplt.ylabel(\"Training (black) and test (blue) MAE (error)\")\n\n\nText(0, 0.5, 'Training (black) and test (blue) MAE (error)')\n\n\n\n\n\n\n\n\n\nThe convergence plot highlights a critical observation: there exists a specific range of min_samples_split values where the model performs well on both the training and test datasets. However, as min_samples_split goes beyond this range, the model’s performance on the test data deteriorates noticeably.\nIn practical terms, this indicates that we should seek a min_samples_split value that does not result in a significant drop in test data performance. The objective is to find the sweet spot where the model maintains good generalization to unseen data without overfitting or underfitting.\n\n\nRe-train with optimal parameters\n\n\nCode\n# INITIALIZE MODEL \nmodel = DecisionTreeClassifier(max_depth=10, min_samples_split=30)\nmodel.fit(x_train,y_train)                     # TRAIN MODEL \n\n\n# OUTPUT PREDICTIONS FOR TRAINING AND TEST SET \nyp_train = model.predict(x_train)\nyp_test = model.predict(x_test)\n\nerr1=mean_absolute_error(y_train, yp_train) \nerr2=mean_absolute_error(y_test, yp_test) \n    \nprint(\" train error:\",err1)\nprint(\" test error:\" ,err2)\n\n\n train error: 0.0008601225674658639\n test error: 0.0034393809114359416\n\n\n\n\nParity Plot\n\nPlotting y_pred vs y_data lets you see how good the fit is\nThe closer to the line y=x the better the fit (ypred=ydata –&gt; prefect fit)\n\n\n\nCode\nplt.plot(y_train,yp_train ,\"o\", color='k')\nplt.plot(y_test,yp_test ,\"o\", color='b')\nplt.plot(y_train,y_train ,\"-\", color='r')\n\nplt.xlabel(\"y_data\")\nplt.ylabel(\"y_pred (blue=test)(black=Train)\")\n\n\nText(0, 0.5, 'y_pred (blue=test)(black=Train)')\n\n\n\n\n\n\n\n\n\n\nTraining Data Fit: The black dots are closer to the red line, indicating a good fit for the training data.\nTest Data Fit: The blue dots are spread out, indicating more variance in the fit for the test data compared to the training data.\nOverall Performance: Since some blue points are far from the red line, it suggests the model may not generalize well to new, unseen data, possibly overfitting to the training data.\n\n\n\nPlot Tree\n\n\nCode\nfrom sklearn import tree\ndef plot_tree(model):\n    fig = plt.figure(figsize=(25,20))\n    _ = tree.plot_tree(model, \n                    filled=True)\n    plt.show()\n\nplot_tree(model)"
  },
  {
    "objectID": "Decision_Trees.html#conclusionsthe-final-reaults-are-discussed-with-each-section",
    "href": "Decision_Trees.html#conclusionsthe-final-reaults-are-discussed-with-each-section",
    "title": "Decision Tree",
    "section": "Conclusions(The final reaults are discussed with each section)",
    "text": "Conclusions(The final reaults are discussed with each section)\n\nThe decision tree with classification analysis gave us insights into heart disease risks, which performed quite well when looking back at the data we trained it on. However, when faced with new, unseen data, the model’s predictions were inconsistent. This suggests that while our model has learned well from past data, it needs to be better tuned to handle new information effectively.\nTo improve, we aim to adjust our model’s complexity. When the model is too simple, and we miss important patterns, when the model is too complex, and we face the risk if overfitting instead of generalization. The goal is to build a model that not only learns from the past but is also ptovide usefull insight for generalization."
  },
  {
    "objectID": "Decision_Trees.html#data-understanding",
    "href": "Decision_Trees.html#data-understanding",
    "title": "Decision Tree",
    "section": "Data understanding",
    "text": "Data understanding\nFor this part of the analysis, I will be using the Framingham Heart Study dataset@RProjectFramingham for both Decision Tree and Random Forest Analysis, the target variable for this analysis would be GLUCOSE, in mmol/L (millimoles per liter).\n\n\nCode\nfrom sklearn.metrics import mean_absolute_percentage_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Load the Frammingham Heart Study data set\ndata = pd.read_csv(\"data/frmgham2.csv\")\nprint(data.head())\ndata.dropna(inplace=True)\n\n# Split target variables and predictor variables\ny = data['GLUCOSE']\nx = data.drop('GLUCOSE', axis=1)\nx = x.iloc[:, 1:]\nx = x.values\ny = y.values\n\n# Normalize \nx=0.1+(x-np.min(x,axis=0))/(np.max(x,axis=0)-np.min(x,axis=0))\ny=0.1+(y-np.min(y,axis=0))/(np.max(y,axis=0)-np.min(y,axis=0))\n\n\n   RANDID  SEX  TOTCHOL  AGE  SYSBP  DIABP  CURSMOKE  CIGPDAY    BMI  \\\n0    2448    1    195.0   39  106.0   70.0         0      0.0  26.97   \n1    2448    1    209.0   52  121.0   66.0         0      0.0    NaN   \n2    6238    2    250.0   46  121.0   81.0         0      0.0  28.73   \n3    6238    2    260.0   52  105.0   69.5         0      0.0  29.43   \n4    6238    2    237.0   58  108.0   66.0         0      0.0  28.50   \n\n   DIABETES  ...  CVD  HYPERTEN  TIMEAP  TIMEMI  TIMEMIFC  TIMECHD  TIMESTRK  \\\n0         0  ...    1         0    8766    6438      6438     6438      8766   \n1         0  ...    1         0    8766    6438      6438     6438      8766   \n2         0  ...    0         0    8766    8766      8766     8766      8766   \n3         0  ...    0         0    8766    8766      8766     8766      8766   \n4         0  ...    0         0    8766    8766      8766     8766      8766   \n\n   TIMECVD  TIMEDTH  TIMEHYP  \n0     6438     8766     8766  \n1     6438     8766     8766  \n2     8766     8766     8766  \n3     8766     8766     8766  \n4     8766     8766     8766  \n\n[5 rows x 39 columns]\n\n\nC:\\Users\\xinzh\\AppData\\Local\\Temp\\ipykernel_35996\\2745761356.py:18: RuntimeWarning:\n\ninvalid value encountered in divide"
  },
  {
    "objectID": "Decision_Trees.html#eda-1",
    "href": "Decision_Trees.html#eda-1",
    "title": "Decision Tree",
    "section": "EDA",
    "text": "EDA\n\n\nCode\ndata_eda = data.iloc[:, 1:]\nprint(data_eda.describe())\n\n\n               SEX      TOTCHOL          AGE        SYSBP        DIABP  \\\ncount  2236.000000  2236.000000  2236.000000  2236.000000  2236.000000   \nmean      1.565742   237.598837    60.166816   139.027728    80.999106   \nstd       0.495770    45.288008     8.295838    22.343693    11.330523   \nmin       1.000000   112.000000    44.000000    86.000000    30.000000   \n25%       1.000000   206.000000    53.000000   122.000000    73.000000   \n50%       2.000000   235.000000    59.000000   136.000000    80.000000   \n75%       2.000000   265.000000    67.000000   153.000000    88.000000   \nmax       2.000000   625.000000    81.000000   246.000000   130.000000   \n\n          CURSMOKE      CIGPDAY          BMI     DIABETES       BPMEDS  ...  \\\ncount  2236.000000  2236.000000  2236.000000  2236.000000  2236.000000  ...   \nmean      0.348837     6.864490    25.752227     0.070662     0.144454  ...   \nstd       0.476709    11.622125     3.883322     0.256317     0.351629  ...   \nmin       0.000000     0.000000    14.430000     0.000000     0.000000  ...   \n25%       0.000000     0.000000    23.187500     0.000000     0.000000  ...   \n50%       0.000000     0.000000    25.380000     0.000000     0.000000  ...   \n75%       1.000000    10.000000    27.845000     0.000000     0.000000  ...   \nmax       1.000000    80.000000    46.520000     1.000000     1.000000  ...   \n\n               CVD     HYPERTEN       TIMEAP       TIMEMI     TIMEMIFC  \\\ncount  2236.000000  2236.000000  2236.000000  2236.000000  2236.000000   \nmean      0.233900     0.738372  7731.038909  8074.634615  8030.673971   \nstd       0.423404     0.439619  2032.270314  1601.230796  1661.646908   \nmin       0.000000     0.000000     0.000000     0.000000     0.000000   \n25%       0.000000     0.000000  7570.750000  8609.500000  8454.000000   \n50%       0.000000     1.000000  8766.000000  8766.000000  8766.000000   \n75%       0.000000     1.000000  8766.000000  8766.000000  8766.000000   \nmax       1.000000     1.000000  8766.000000  8766.000000  8766.000000   \n\n           TIMECHD     TIMESTRK      TIMECVD      TIMEDTH      TIMEHYP  \ncount  2236.000000  2236.000000  2236.000000  2236.000000  2236.000000  \nmean   7498.554562  8167.498211  7640.489267  8351.642665  3943.845707  \nstd    2255.888945  1342.934286  2138.744071   969.038296  3495.722584  \nmin       0.000000     0.000000     0.000000  4182.000000     0.000000  \n25%    6921.250000  8673.250000  7347.500000  8766.000000     0.000000  \n50%    8766.000000  8766.000000  8766.000000  8766.000000  2973.500000  \n75%    8766.000000  8766.000000  8766.000000  8766.000000  7989.250000  \nmax    8766.000000  8766.000000  8766.000000  8766.000000  8766.000000  \n\n[8 rows x 38 columns]\n\n\n\nCorrelation Matrix Heatmap\n\n\nCode\ncorr = data.corr()\nsns.set_theme(style=\"white\")\nf, ax = plt.subplots(figsize=(11, 9))  # Set up the matplotlib figure\ncmap = sns.diverging_palette(230, 20, as_cmap=True)     # Generate a custom diverging colormap\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr,  cmap=cmap, vmin=-1, vmax=1, center=0,\n        square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show();"
  },
  {
    "objectID": "Decision_Trees.html#model-tuning-1",
    "href": "Decision_Trees.html#model-tuning-1",
    "title": "Decision Tree",
    "section": "Model Tuning",
    "text": "Model Tuning\n\nPartion data to train/test split\n\n\nCode\nfrom sklearn.model_selection import train_test_split\ntest_ratio=0.2\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_ratio, random_state=0)\ny_train=y_train.flatten()\ny_test=y_test.flatten()\n\nprint(\"x_train.shape        :\",x_train.shape)\nprint(\"y_train.shape        :\",y_train.shape)\n\nprint(\"X_test.shape     :\",x_test.shape)\nprint(\"y_test.shape     :\",y_test.shape)\n\n\nx_train.shape        : (1788, 37)\ny_train.shape        : (1788,)\nX_test.shape     : (448, 37)\ny_test.shape     : (448,)\n\n\n\n\nHyper-Parameter tuning for Decision Tree Classification ((max_depth))\n\n\nCode\nfrom sklearn.metrics import mean_absolute_percentage_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.tree import DecisionTreeClassifier\n\n\n# HYPER PARAMETER SEARCH FOR OPTIMAL NUMBER OF NEIGHBORS \nhyper_param=[]\ntrain_error=[]\ntest_error=[]\n\n# LOOP OVER HYPER-PARAM\nfor i in range(1,40):\n    # INITIALIZE MODEL \n    model = DecisionTreeRegressor(max_depth=i)\n\n    # TRAIN MODEL \n    model.fit(x_train,y_train)\n\n    # OUTPUT PREDICTIONS FOR TRAINING AND TEST SET \n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n\n    # shift=1+np.min(y_train) #add shift to remove division by zero \n    err1=mean_absolute_error(y_train, yp_train) \n    err2=mean_absolute_error(y_test, yp_test) \n    \n    # err1=100.0*np.mean(np.absolute((yp_train-y_train)/y_train))\n    # err2=100.0*np.mean(np.absolute((yp_test-y_test)/y_test))\n\n    hyper_param.append(i)\n    train_error.append(err1)\n    test_error.append(err2)\n\n    if(i==1 or i%10==0):\n        print(\"hyperparam =\",i)\n        print(\" train error:\",err1)\n        print(\" test error:\" ,err2)\n\n\nhyperparam = 1\n train error: 0.035268787108589356\n test error: 0.03418032867241032\nhyperparam = 10\n train error: 0.02038622946162721\n test error: 0.04358912209657024\nhyperparam = 20\n train error: 0.005180126792851705\n test error: 0.047937823543169375\nhyperparam = 30\n train error: 0.00022663570019033476\n test error: 0.04882001990611059\n\n\n\nFor max_depth = 1, both the training and test errors are relatively low, with values around 0.03. This suggests that the decision tree is fitting the data well.\nAs the max_depth increase to 10 and beyond, the training erros starts to drop, however, the test error starts to go up.\nInterestingly, as max_depth furthur increase to 20 and 30, the training error remains very low (close to 0), but the test error starts to increase slightly. This is a sign of overfitting. The model is capturing the data too well, we don’t want this to happen for generalization purposes.\nOverall, the results show that a max_depth of around 1 to 10 appears to be a good choice because it achieves low test error without overfitting the training data.\nConvergence plot\n\n\n\nCode\nplt.plot(hyper_param,train_error ,linewidth=2, color='k')\nplt.plot(hyper_param,test_error ,linewidth=2, color='b')\n\nplt.xlabel(\"Depth of tree (max depth)\")\nplt.ylabel(\"Training (black) and test (blue) MAE (error)\")\n\ni=1\nprint(hyper_param[i],train_error[i],test_error[i])\n\n\n2 0.03442784374001432 0.032813904712789006\n\n\n\n\n\n\n\n\n\n\nThe convergence plot clearly demonstrates that as the max_depth hyperparameter increases, The point at which the lines start to diverge significantly is critical; it marks the transition from underfitting to the optimal complexity and then to overfitting. According to the plot a max_depth of 0 to 5 is optimal for a balance between training error and testing error.\n\n\n\nHyper-Parameter tuning for Decision Tree Regression (min_samples_splitint)\n\n\nCode\n# HYPER PARAMETER SEARCH FOR OPTIMAL NUMBER OF NEIGHBORS \nhyper_param=[]\ntrain_error=[]\ntest_error=[]\n\n# LOOP OVER HYPER-PARAM\nfor i in range(2,100):\n    # INITIALIZE MODEL \n    model = DecisionTreeRegressor(min_samples_split=i)\n\n    # TRAIN MODEL \n    model.fit(x_train,y_train)\n\n    # OUTPUT PREDICTIONS FOR TRAINING AND TEST SET \n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n\n    # shift=1+np.min(y_train) #add shift to remove division by zero \n    err1=mean_absolute_error(y_train, yp_train) \n    err2=mean_absolute_error(y_test, yp_test) \n    \n    # err1=100.0*np.mean(np.absolute((yp_train-y_train)/y_train))\n    # err2=100.0*np.mean(np.absolute((yp_test-y_test)/y_test))\n\n    hyper_param.append(i)\n    train_error.append(err1)\n    test_error.append(err2)\n\n    if(i%10==0):\n        print(\"hyperparam =\",i)\n        print(\" train error:\",err1)\n        print(\" test error:\" ,err2)\n\n\nhyperparam = 10\n train error: 0.014984530706046054\n test error: 0.045871667532753846\nhyperparam = 20\n train error: 0.021962415929112074\n test error: 0.043417885579028495\nhyperparam = 30\n train error: 0.026579851814848933\n test error: 0.040156857780947484\nhyperparam = 40\n train error: 0.0279409667047845\n test error: 0.038751571864489465\nhyperparam = 50\n train error: 0.028391822245632067\n test error: 0.038300992283321324\nhyperparam = 60\n train error: 0.029240913243283456\n test error: 0.036936685875858556\nhyperparam = 70\n train error: 0.029627133194660586\n test error: 0.036694713507271985\nhyperparam = 80\n train error: 0.03032461525639734\n test error: 0.03656614931125601\nhyperparam = 90\n train error: 0.03058836291359766\n test error: 0.036432623399717076\n\n\n\nThe min_samples_split parameter controls the minimum number of samples required to split an internal node during the construction of the decision tree.\nAs min_samples_split increases from 10 to 20, both the training and test errors generally decrease. This suggests that when internal nodes require a larger number of samples to split.\nBeyond min_samples_split = 20, the training error remains low, indicating that the model still fits the training data well, the test error drops at the same time.\n\nConvergence Plot\n\n\nCode\nplt.plot(hyper_param,train_error ,linewidth=2, color='k')\nplt.plot(hyper_param,test_error ,linewidth=2, color='b')\n\nplt.xlabel(\"Minimum number of points in split (min_samples_split)\")\nplt.ylabel(\"Training (black) and test (blue) MAE (error)\")\n\n\nText(0, 0.5, 'Training (black) and test (blue) MAE (error)')\n\n\n\n\n\n\n\n\n\nThe convergence plot demonstrates that test error and train error converge to a point, roughly min_samples_spliy = 50.\n\n\nRe-train with optimal parameters\n\n\nCode\n# INITIALIZE MODEL \nmodel = DecisionTreeRegressor(max_depth=1,min_samples_split=50)\nmodel.fit(x_train,y_train)                     # TRAIN MODEL \n\n\n# OUTPUT PREDICTIONS FOR TRAINING AND TEST SET \nyp_train = model.predict(x_train)\nyp_test = model.predict(x_test)\n\nerr1=mean_absolute_error(y_train, yp_train) \nerr2=mean_absolute_error(y_test, yp_test) \n    \nprint(\" train error:\",err1)\nprint(\" test error:\" ,err2)\n\n\n train error: 0.03526878710858939\n test error: 0.03418032867241034\n\n\n\n\nParity Plot\n\nPlotting y_pred vs y_data lets you see how good the fit is\nThe closer to the line y=x the better the fit (ypred=ydata –&gt; prefect fit)\n\n\n\nCode\nplt.plot(y_train,yp_train ,\"o\", color='k')\nplt.plot(y_test,yp_test ,\"o\", color='b')\nplt.plot(y_train,y_train ,\"-\", color='r')\n\nplt.xlabel(\"y_data\")\nplt.ylabel(\"y_pred (blue=test)(black=Train)\")\n\n\nText(0, 0.5, 'y_pred (blue=test)(black=Train)')\n\n\n\n\n\n\n\n\n\n\n\nPlot Tree\n\n\nCode\nfrom sklearn import tree\ndef plot_tree(model):\n    fig = plt.figure(figsize=(25,20))\n    _ = tree.plot_tree(model, \n                    filled=True)\n    plt.show()\n\nplot_tree(model)"
  },
  {
    "objectID": "Decision_Trees.html#conclusionsthe-final-reaults-are-discussed-with-each-section-1",
    "href": "Decision_Trees.html#conclusionsthe-final-reaults-are-discussed-with-each-section-1",
    "title": "Decision Tree",
    "section": "Conclusions(The final reaults are discussed with each section)",
    "text": "Conclusions(The final reaults are discussed with each section)\n\nThe decision tree with regression analysis gave us insights into heart disease risks, which performed quite well when looking back at the data we trained it on. However, when faced with new, unseen data, the model’s predictions were inconsistent. This suggests that while our model has learned well from past data, it needs to be better tuned to handle new information effectively.\nTo improve, we aim to adjust our model’s complexity. When the model is too simple, and we miss important patterns, when the model is too complex, and we face the risk if overfitting instead of generalization. The goal is to build a model that not only learns from the past but is also ptovide usefull insight for generalization."
  },
  {
    "objectID": "Data_Cleaning.html",
    "href": "Data_Cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "The availability and utilization of high-quality data is a crucial part of a successful data science project. For Chronic diseases, which represent a growing public health challenge around the world, require comprehensive and accurate data to faciliate analysis and decison-making. However, real-world healthcare datasets usually come with lots of inconsistency, those including missing values, and errors that can leads to bias and incorrect conclusions. In this section, I am going to perfrom a critical process of data cleaning, where we utilize various techniques and methodologies to solve these issues and transform our dataset into a reliable and ready-for-analysis dataset. By addressing data quality concerns, we have the foundation for reliable epidemiological investigations."
  },
  {
    "objectID": "Data_Cleaning.html#introduction",
    "href": "Data_Cleaning.html#introduction",
    "title": "Data Cleaning",
    "section": "",
    "text": "The availability and utilization of high-quality data is a crucial part of a successful data science project. For Chronic diseases, which represent a growing public health challenge around the world, require comprehensive and accurate data to faciliate analysis and decison-making. However, real-world healthcare datasets usually come with lots of inconsistency, those including missing values, and errors that can leads to bias and incorrect conclusions. In this section, I am going to perfrom a critical process of data cleaning, where we utilize various techniques and methodologies to solve these issues and transform our dataset into a reliable and ready-for-analysis dataset. By addressing data quality concerns, we have the foundation for reliable epidemiological investigations."
  },
  {
    "objectID": "Data_Cleaning.html#data-cleaning-of-the-u.s.-chronic-disease-indicators-cdi",
    "href": "Data_Cleaning.html#data-cleaning-of-the-u.s.-chronic-disease-indicators-cdi",
    "title": "Data Cleaning",
    "section": "Data cleaning of the U.S. Chronic Disease Indicators (CDI)",
    "text": "Data cleaning of the U.S. Chronic Disease Indicators (CDI)\n\nLoad the raw data\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(DT)\n#Load the data\nus_chronic &lt;- read_csv(\"data/U.S._Chronic_Disease_Indicators__CDI_.csv\")\n\n\nRows: 1185676 Columns: 34\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (19): LocationAbbr, LocationDesc, DataSource, Topic, Question, DataValue...\ndbl  (5): YearStart, YearEnd, DataValueAlt, LowConfidenceLimit, HighConfiden...\nlgl (10): Response, StratificationCategory2, Stratification2, Stratification...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\na &lt;- head(us_chronic)\ndatatable(a)\n\n\n\n\n\n\n\n\nChecking for different chronic diseases types\n\n\nCode\n# Check the categories of different disease incidents\nus_chronic$Topic &lt;- as.factor(us_chronic$Topic)\ncategory &lt;- levels(us_chronic$Topic)\ncategory\n\n\n [1] \"Alcohol\"                                        \n [2] \"Arthritis\"                                      \n [3] \"Asthma\"                                         \n [4] \"Cancer\"                                         \n [5] \"Cardiovascular Disease\"                         \n [6] \"Chronic Kidney Disease\"                         \n [7] \"Chronic Obstructive Pulmonary Disease\"          \n [8] \"Diabetes\"                                       \n [9] \"Disability\"                                     \n[10] \"Immunization\"                                   \n[11] \"Mental Health\"                                  \n[12] \"Nutrition, Physical Activity, and Weight Status\"\n[13] \"Older Adults\"                                   \n[14] \"Oral Health\"                                    \n[15] \"Overarching Conditions\"                         \n[16] \"Reproductive Health\"                            \n[17] \"Tobacco\"                                        \n\n\n\n\nChecking for NA columns\n\n\nCode\n# Check for NA values in each column\nna_columns &lt;- colSums(is.na(us_chronic))\n\n# Display columns with NA values\nprint(na_columns)\n\n\n                YearStart                   YearEnd              LocationAbbr \n                        0                         0                         0 \n             LocationDesc                DataSource                     Topic \n                        0                         0                         0 \n                 Question                  Response             DataValueUnit \n                        0                   1185676                    152123 \n            DataValueType                 DataValue              DataValueAlt \n                        0                    378734                    381098 \n  DataValueFootnoteSymbol         DatavalueFootnote        LowConfidenceLimit \n                   791966                    791966                    503296 \n      HighConfidenceLimit   StratificationCategory1           Stratification1 \n                   503296                         0                         0 \n  StratificationCategory2           Stratification2   StratificationCategory3 \n                  1185676                   1185676                   1185676 \n          Stratification3               GeoLocation                ResponseID \n                  1185676                     10166                   1185676 \n               LocationID                   TopicID                QuestionID \n                        0                         0                         0 \n          DataValueTypeID StratificationCategoryID1         StratificationID1 \n                        0                         0                         0 \nStratificationCategoryID2         StratificationID2 StratificationCategoryID3 \n                  1185676                   1185676                   1185676 \n        StratificationID3 \n                  1185676 \n\n\nAs we can see, variables StratificationCategory2, Stratification2, StratificationCategory3, Stratification3, ResponseID, StratificationCategoryID2, StratificationID2, StratificationCategoryID3, StratificationID3 are completley filled with NA values. We need to drop the columns with completely NA values.\n\n\nCode\n#Drop the columns with completely NA values\nus_chronic &lt;- us_chronic[, colSums(is.na(us_chronic)) != nrow(us_chronic)]\ndatatable(head(us_chronic))\n\n\n\n\n\n\n\n\nTransforming data to tidy\n\nLet’s first look at what is inside StratificationCategory1 and Stratification1.\n\n\n\nCode\n#Loop through each unique category\ntypes_s &lt;- unique(us_chronic$StratificationCategory1)\nfor (i in types_s) {\n  cat(\"Category:\", i, \"\\n\")\n  \n# Get unique Types for the current category\n  types &lt;- unique(us_chronic$Stratification1[us_chronic$StratificationCategory1 == i])\n  \n # Print unique types\n cat(\"Types:\\n\", paste(types, collapse = \"\\n\"), \"\\n\\n\")\n}\n\n\nCategory: Race/Ethnicity \nTypes:\n American Indian or Alaska Native\nAsian, non-Hispanic\nWhite, non-Hispanic\nHispanic\nBlack, non-Hispanic\nAsian or Pacific Islander\nMultiracial, non-Hispanic\nOther, non-Hispanic \n\nCategory: Gender \nTypes:\n Female\nMale \n\nCategory: Overall \nTypes:\n Overall \n\n\n\nIn order to transform data to tidy, I need three column variables specify Race, Gender and Overall.\n\n\n\nCode\nus_chronic_tidy &lt;- us_chronic %&gt;%\n  mutate(Race = ifelse(StratificationCategory1 == \"Race/Ethnicity\", Stratification1, NA),\n         Gender = ifelse(StratificationCategory1 == \"Gender\", Stratification1, NA),\n         Overall = ifelse(StratificationCategory1 == \"Overall\", Stratification1, NA)) %&gt;%\n  \n  select(-StratificationCategory1, -Stratification1) %&gt;%\n  \n  mutate(RaceID = ifelse(StratificationCategoryID1 == \"RACE\", StratificationID1, NA),\n         GenderID = ifelse(StratificationCategoryID1 == \"GENDER\", StratificationID1, NA),\n         OverallID = ifelse(StratificationCategoryID1 == \"OVERALL\", StratificationID1, NA)) %&gt;%\n  \n  select(-StratificationCategoryID1, -StratificationID1)\n\n# View the resulting tidy dataset\ndatatable(head(us_chronic_tidy))\n\n\n\n\n\n\n\n\nSave the cleaned data\n\n\nCode\nwrite.csv(us_chronic_tidy,file='data/us_chronic.csv', row.names = FALSE)"
  },
  {
    "objectID": "Conclusions.html#conclusion",
    "href": "Conclusions.html#conclusion",
    "title": "Conclusions",
    "section": "Conclusion",
    "text": "Conclusion\n\nNaive Bayes\nThe first model employed in our analysis, the Naive Bayes classifier, demonstrated exceptional performance in classifying cardiovascular diseases (CVD). It achieved high accuracy on both the training and validation datasets, with the validation accuracy surpassing that of the training set, indicating strong generalization to unseen data. Additionally, the model excelled in precision, recall, and F1 scores, showcasing its ability to maintain a balanced performance by effectively predicting both true positives and true negatives. The ROC curve analysis further reinforced its robustness, with an impressive Area Under the Curve (AUC) of 0.97, highlighting its excellent ability to distinguish between classes. These results underscore the Naive Bayes classifier’s potential as a reliable and efficient tool for medical diagnostics, particularly in the context of cardiovascular disease classification.\n\n\nClustering\nThe second part of our analysis focused on evaluating three clustering models: K-Means, DBSCAN, and Hierarchical Clustering. Of these, K-Means proved to be moderately effective, showing a reasonable balance between simplicity and performance. In contrast, DBSCAN and Hierarchical Clustering encountered challenges in achieving high precision and accurately distinguishing between classes, as reflected in their lower performance metrics. Despite its limitations, K-Means emerged as the preferred model in this context due to its straightforward implementation and relatively better effectiveness in clustering the data.\n\n\nDecision tress\nThe Decision Tree model was applied to both classification and regression tasks, providing valuable insights into heart disease risks and glucose level predictions. While the model performed well on the training data, its performance on unseen data revealed some variability, indicating the need for further tuning to optimize its predictive capabilities. The primary goal is to strike a balance in model complexity, ensuring it effectively captures essential patterns in the data without overfitting. This approach aims to enhance the model’s ability to make accurate and reliable predictions for new data, improving its overall utility in medical diagnostics and health monitoring\n\n\nNLP\nThe analysis of the r/diabetes subreddit reveals critical insights into community discussions surrounding diabetes. Using TF-IDF, we identified key terms such as “insulin,” “eat,” “sugar,” “diabetes,” “carb,” and “blood,” which highlight two major thematic categories: medications (various drug classes) and lifestyle (diet, exercise, and symptom management). Sentiment analysis further demonstrates that discussions about lifestyle changes, such as diet and exercise, generally elicit positive sentiment, reflecting a community preference for non-pharmacological interventions, which may pose challenges for medication adherence among those requiring them. On the other hand, sentiments around medications are predominantly negative, although certain classes like alpha-glucosidase inhibitors (“alpha inhibitors”) stand out for their surprisingly high positive sentiment. In contrast, Semaglutide and related GLP-1 agonists are less favored, potentially due to concerns about side effects or costs. The data also illustrate varied sentiment across different diabetes types (Type 1, Type 2, and gestational) and their common symptoms, such as fatigue, thirst, blurred vision, and frequent urination, with gestational diabetes and blurred vision often viewed negatively. Notably, a sharp increase in positive sentiment in December 2023 aligns with groundbreaking research developments, such as newly discovered enzyme targets for insulin production, sparking optimism within the community. This comprehensive analysis sheds light on the community’s evolving priorities and perspectives on diabetes management."
  },
  {
    "objectID": "Conclusions.html#insights-for-future-work",
    "href": "Conclusions.html#insights-for-future-work",
    "title": "Conclusions",
    "section": "Insights for future work",
    "text": "Insights for future work\nBuilding on the insights gained from our analysis, several avenues for future work can be explored to enhance the utility and impact of our models and methodologies. For the Naive Bayes classifier, further research could focus on applying the model to larger and more diverse datasets to validate its robustness and generalizability in cardiovascular disease (CVD) classification. Additionally, exploring ensemble methods that combine Naive Bayes with other classifiers could further improve performance metrics, particularly in edge cases or underrepresented patient subgroups.\nIn the clustering models, future efforts could involve the incorporation of advanced techniques, such as density-based methods with optimized hyperparameters or hybrid models that combine K-Means with Hierarchical Clustering or DBSCAN. These approaches could address current limitations in distinguishing complex patterns, especially in datasets with overlapping classes. Additionally, integrating domain-specific features or embeddings could improve the precision of clustering results and provide deeper insights into latent structures within the data.\nFor the Decision Tree model, future work could focus on advanced tree-based methods such as Random Forests or Gradient Boosted Trees to address the variability observed in model performance between training and unseen data. Hyperparameter optimization and feature selection could also help balance complexity and generalization. Moreover, the deployment of the Decision Tree model in real-world settings, such as mobile health applications or diagnostic platforms, could provide valuable feedback for refining its predictive capabilities in both classification and regression tasks.\nIn the realm of NLP, expanding the analysis of r/diabetes to include additional features such as user demographics or temporal trends in posting behavior could offer more granular insights into community dynamics. Leveraging transformer-based architectures like BERT or GPT could significantly enhance the accuracy of topic modeling and sentiment analysis, particularly for nuanced or context-dependent discussions. Additionally, the integration of external datasets, such as medical research publications or clinical guidelines, could provide context-aware insights and improve the interpretability of model outputs. Longitudinal studies analyzing how sentiments and discussions evolve in response to medical advancements or public health events could further enrich our understanding of patient communities.\nLastly, a unifying effort could involve integrating all these models into a comprehensive framework that combines classification, clustering, regression, and NLP insights. This holistic approach could be applied across diverse medical domains, enabling researchers and healthcare practitioners to better understand patient behaviors, improve diagnostic accuracy, and tailor interventions. Future work will also emphasize the ethical considerations of using machine learning in healthcare, ensuring models are explainable, equitable, and privacy-preserving to maximize their societal benefit."
  },
  {
    "objectID": "Clustering_Python.html",
    "href": "Clustering_Python.html",
    "title": "Clustering",
    "section": "",
    "text": "Code\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\nfrom sklearn.metrics import silhouette_score\nfrom scipy.spatial.distance import cdist\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nfrom scipy.optimize import linear_sum_assignment\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\nfrom scipy.cluster.hierarchy import dendrogram, linkage"
  },
  {
    "objectID": "Clustering_Python.html#introduction",
    "href": "Clustering_Python.html#introduction",
    "title": "Clustering",
    "section": "Introduction",
    "text": "Introduction\nIn the field of chronic diseases research, understanding complex diseases like diabetes often involves discerning hidden patterns in data. This project utilizes clustering techniques, specifically K-Means, DBSCAN, and Hierarchical clustering, to analyze Diabetes from the Framingham Heart Study R Project (n.d.). The use of these diverse methods allows for a comprehensive exploration of diabetes prediction and understanding. Each method, with its unique approach, helps in identifying distinct groups and complex relationships among various factors. By focusing on diabetes as the core subject, this analysis aims to decode how different risk factors, demographics, and lifestyle choices intersect, contributing to the disease’s development and progression."
  },
  {
    "objectID": "Clustering_Python.html#theory",
    "href": "Clustering_Python.html#theory",
    "title": "Clustering",
    "section": "Theory",
    "text": "Theory\n\nK Means Clustering\n\nK-means clustering divides data into k distinct groups. Initially, it randomly selects centroids from the data, which are like central points for each group. Data points are then grouped with the nearest centroid, forming initial clusters. After this, the average position of all the points in each cluster is calculated, creating new centroids. This process of assigning data points to the nearest centroid and recalculating the centroids is repeated until the clusters stabilize, usually meaning they don’t change much between iterations.\n\nFor evaluating how well the clustering worked, two common methods are the Elbow Method and Silhouette Analysis. The Elbow Method helps to choose a suitable number of clusters (k). It involves plotting the Sum of Squared Distances also known as inertia between data points and their respective cluster centroids against a range of k values. The elbow point, where the rate of decrease sharply changes, suggests a good number of clusters. On the other hand, Silhouette Analysis measures how similar a data point is to its own cluster compared to other clusters. It helps understand how well-separated the clusters are, with higher silhouette scores indicating better-defined clusters. Dabbura (2023)\n\n\nDBSAN\n\nDBSCAN looks at clustering from a density point of view. It groups points that are closely packed together, marking points that are too far from any cluster as outliers. The process starts by picking a point and finding all points within a certain distance, then a cluster is formed if there are enough pints. DBSCAN then checks the points around these new points, expanding the cluster if necessary. This method is particularly useful for data with irregular shapes. The key parameters are the maximum distance (also know as eps which is the radius of the neighborhood around each data point.) between points to be considered neighbors and the minimum number of points to form a dense region. Unlike K-Means, specifying the number of clusters is not required.\nFor evaluating DBSCAN, using the Silhouette score is more appropriate than the Elbow method. The Elbow method is typically used for K Means Clustering where you need to determine the optimal number of clusters. In contrast, DBSCAN does not require specifying the number of clusters. The Silhouette score, which measures the quality of clusters by how well-separated they are, is suitable for evaluating the model performance of DBSCAN.\n\n\n\nHierarchical clustering\n\nHierarchical clustering builds a tree of clusters by progressively linking together data points or existing clusters. Initially, each point is a cluster. The closest pairs of points (or clusters) are then merged into new clusters. This process is repeated, gradually forming larger clusters. The result can be visualized as a tree or dendrogram, showing the hierarchy of clustering, it is simlilar to the structure of a decision tree. An advantage of hierarchical clustering is that we don’t need to decide the number of clusters in advance. We can choose the number of clusters by chopping the dendrogram at the desired level.\nFor hierarchical clustering, the Silhouette score is a better method. While the Elbow method is useful for algorithms like K-means where you need to predetermine the number of clusters, hierarchical clustering provides a tree-like structure of data. The Silhouette score can help in determining the quality of the clusters formed at different levels. This method offers a clear insight into the distinctiveness of the clusters formed."
  },
  {
    "objectID": "Clustering_Python.html#methods",
    "href": "Clustering_Python.html#methods",
    "title": "Clustering",
    "section": "Methods",
    "text": "Methods\n\nData preparation\n\n\nCode\n# Load the Frammingham Heart Study data set\ndata = pd.read_csv(\"data/frmgham2.csv\")\ndata.head()\n\n\n\n\n\n\n\n\n\nRANDID\nSEX\nTOTCHOL\nAGE\nSYSBP\nDIABP\nCURSMOKE\nCIGPDAY\nBMI\nDIABETES\n...\nCVD\nHYPERTEN\nTIMEAP\nTIMEMI\nTIMEMIFC\nTIMECHD\nTIMESTRK\nTIMECVD\nTIMEDTH\nTIMEHYP\n\n\n\n\n0\n2448\n1\n195.0\n39\n106.0\n70.0\n0\n0.0\n26.97\n0\n...\n1\n0\n8766\n6438\n6438\n6438\n8766\n6438\n8766\n8766\n\n\n1\n2448\n1\n209.0\n52\n121.0\n66.0\n0\n0.0\nNaN\n0\n...\n1\n0\n8766\n6438\n6438\n6438\n8766\n6438\n8766\n8766\n\n\n2\n6238\n2\n250.0\n46\n121.0\n81.0\n0\n0.0\n28.73\n0\n...\n0\n0\n8766\n8766\n8766\n8766\n8766\n8766\n8766\n8766\n\n\n3\n6238\n2\n260.0\n52\n105.0\n69.5\n0\n0.0\n29.43\n0\n...\n0\n0\n8766\n8766\n8766\n8766\n8766\n8766\n8766\n8766\n\n\n4\n6238\n2\n237.0\n58\n108.0\n66.0\n0\n0.0\n28.50\n0\n...\n0\n0\n8766\n8766\n8766\n8766\n8766\n8766\n8766\n8766\n\n\n\n\n5 rows × 39 columns\n\n\n\n\n\nMissing data handelling and normalization\n\n\nCode\n# Check for missing data and calculate percentages\nmissing_data = data.isnull().mean()\n\n# Drop columns with more than 50% missing data\nthreshold = 0.5\ncolumns_to_drop = missing_data[missing_data &gt; threshold].index\ndata.drop(columns=columns_to_drop, inplace=True)\ndata = data.iloc[:, 1:]\n\n# Remove rows with missing data as their percentages are low\ndata.dropna(inplace=True)\n\n# Set target variables\ntarget = data.pop('DIABETES')\n\n# Scale the data\nscaler = StandardScaler()\ndata_scaled = scaler.fit_transform(data)\n\n\n\n\nK-Means Clustering\nHyperparameter tuning function\n\n\nCode\nimport sklearn.cluster\n\n# THIS WILL ITERATE OVER ONE HYPER-PARAMETER (GRID SEARCH)\n# AND RETURN THE CLUSTER RESULT THAT OPTIMIZES THE SILHOUETTE SCORE\ndef maximize_silhouette(X,algo=\"birch\",nmax=20,i_plot=False):\n\n    # PARAM\n    i_print=False\n\n    #FORCE CONTIGUOUS\n    X=np.ascontiguousarray(X)\n\n    # LOOP OVER HYPER-PARAM\n    params=[]; sil_scores=[]\n    sil_max=-10\n    for param in range(2,nmax+1):\n        if(algo==\"birch\"):\n            model = sklearn.cluster.Birch(n_clusters=param).fit(X)\n            labels=model.predict(X)\n\n        if(algo==\"ag\"):\n            model = sklearn.cluster.AgglomerativeClustering(n_clusters=param).fit(X)\n            labels=model.labels_\n\n        if(algo==\"dbscan\"):\n            param=0.25*(param-1)\n            model = sklearn.cluster.DBSCAN(eps=param).fit(X)\n            labels=model.labels_\n\n        if(algo==\"kmeans\"):\n            model = sklearn.cluster.KMeans(n_clusters=param).fit(X)\n            labels=model.predict(X)\n\n        try:\n            sil_scores.append(sklearn.metrics.silhouette_score(X,labels))\n            params.append(param)\n        except:\n            continue\n\n        if(i_print): print(param,sil_scores[-1])\n\n        if(sil_scores[-1]&gt;sil_max):\n             opt_param=param\n             sil_max=sil_scores[-1]\n             opt_labels=labels\n\n    print(\"OPTIMAL PARAMETER =\",opt_param)\n\n    if(i_plot):\n        fig, ax = plt.subplots()\n        ax.plot(params, sil_scores, \"-o\")\n        ax.set(xlabel='Hyper-parameter', ylabel='Silhouette')\n        plt.show()\n\n    return opt_labels\n\n\nUtility Plot function\n\n\nCode\n# UTILITY PLOTTING FUNCTION\ndef plot(X,color_vector):\n    fig, ax = plt.subplots()\n    ax.scatter(X[:,0], X[:,1],c=color_vector, alpha=0.5) #, c=y\n    ax.set(xlabel='Feature-1 (x_1)', ylabel='Feature-2 (x_2)',\n    title='Cluster data')\n    ax.grid()\n    # fig.savefig(\"test.png\")\n    plt.show()\n\n\nElbow Method\n\n\nCode\n# Elbow method for K Means\ninertia = []\nK = range(1, 11)\nfor k in K:\n    kmeanModel = KMeans(n_clusters=k)\n    kmeanModel.fit(data_scaled)\n    inertia.append(kmeanModel.inertia_)\n\n# Plotting the elbow method\nsns.lineplot(x=K, y=inertia, marker=\"o\")\nplt.title('Elbow Method For Optimal k')\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia')\nplt.show()\n\n\n\n\n\n\n\n\n\nSilhouette Method\n\n\nCode\nopt_labels=maximize_silhouette(data_scaled,algo=\"kmeans\",nmax=15, i_plot=True)\nplot(data_scaled,opt_labels)\n\n\nOPTIMAL PARAMETER = 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs we can see from the both of the plots, K=4 would be an optimal clusters value based on the elbow method, as it the inertia tend to get stabilized after k=4. From the Silhouette method, the resulting plot indicates that k=2 is the optimal k since, it has the highest Silhouette value, indicating a well formed clustering structure. Overall, I pick k=2 since at k=2, it is optimal for a binary target outcome.\n\nFinal Results for optimal K of K-Means\n\n\nCode\n# Confusion Matrix Heatmap, used chatgpt as an assitance, this heat map works well\ndef plot_confusion_matrix(cm, class_names):\n    df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n    fig = plt.figure(figsize=(5,4))\n    try:\n        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\", cmap='Blues')\n    except ValueError:\n        raise ValueError(\"Confusion matrix values must be integers.\")\n    \n    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=14)\n    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=14)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    return fig\n\n\n# Final K-Means Clustering with Optimal k\nkmeans_final = KMeans(n_clusters=2)\nkmeans_clusters = kmeans_final.fit_predict(data_scaled)\nclass_names = ['Negative', 'Positive']\n\n# Plot confusion matrix for K-Means\ncm_kmeans = confusion_matrix(target, kmeans_clusters)\nfig_kmeans = plot_confusion_matrix(cm_kmeans, class_names)\nplt.title('Confusion Matrix for K-Means')\nplt.show()\n\nprint(cm_kmeans)\n\nprecision = precision_score(target, kmeans_clusters)\nrecall = recall_score(target, kmeans_clusters)\nf1 = f1_score(target, kmeans_clusters)\naccuracy = accuracy_score(target, kmeans_clusters)\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1-Score: {f1:.4f}\")\n\n\n\n\n\n\n\n\n\n[[2363 6555]\n [ 242  150]]\nAccuracy: 0.2699\nPrecision: 0.0224\nRecall: 0.3827\nF1-Score: 0.0423\n\n\n\nAccuracy (73.01%): This suggests that the model correctly predicts whether a case is positive or negative about 73% of the time.\nPrecision (9.29%): Precision is very low, indicating that when the model predicts a case as positive, it is correct only about 9% of the time.\nRecall (61.73%): The model has a moderately high recall, meaning it correctly identifies approximately 62% of all actual positive cases. However, due to the low precision, many of the positive predictions are not accurate.\nF1-Score (16.15%): The F1-score is quite low because it is the harmonic mean of precision and recall. The low precision adversely affects the F1-score even though the recall is not as low."
  },
  {
    "objectID": "Clustering_Python.html#dbscan",
    "href": "Clustering_Python.html#dbscan",
    "title": "Clustering",
    "section": "DBSCAN",
    "text": "DBSCAN\nSilhouette Method\n\n\nCode\nopt_labels=maximize_silhouette(data_scaled,algo=\"dbscan\",nmax=15, i_plot=True)\nplot(data_scaled,opt_labels)\n\n\nOPTIMAL PARAMETER = 3.5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe optimal parameter for DBSCAN method is 3.5, at eps = 3.5, the Silhouette score is maximized.\n\nFinal Results for optimal eps of DBSCAN\n\n\nCode\noptimal_eps = 3.5\ndbscan_final = DBSCAN(eps=optimal_eps)\ndbscan_clusters = dbscan_final.fit_predict(data_scaled)\n\ncluster_positive_ratio = []\nunique_clusters = set(dbscan_clusters) - {-1}  # Exclude noise if present\n\n# For this section of code, I used chatgpt to help me understand how to convert multi-dimensional data to binary predictions.\n\nfor cluster in unique_clusters:\n    # Create a mask for the current cluster\n    cluster_mask = dbscan_clusters == cluster\n    # Calculate the ratio of positive instances in this cluster\n    positive_ratio = target[cluster_mask].mean()\n    cluster_positive_ratio.append((cluster, positive_ratio))\n\n# Sort clusters by positive ratio and get the cluster with the highest positive ratio\noptimal_cluster = sorted(cluster_positive_ratio, key=lambda x: x[1], reverse=True)[0][0]\n\n# Map the DBSCAN clusters to binary predictions\nbinary_predictions = (dbscan_clusters == optimal_cluster).astype(int)\n\n# Compute metrics\nprecision = precision_score(target, binary_predictions)\nrecall = recall_score(target, binary_predictions)\nf1 = f1_score(target, binary_predictions)\naccuracy = accuracy_score(target, binary_predictions)\n\n# Confusion Matrix\ncm = confusion_matrix(target, binary_predictions)\nfig_kmeans = plot_confusion_matrix(cm, class_names)\nplt.title('Confusion Matrix for DBSCAN')\nplt.show()\n\n# Print metrics\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1-Score: {f1:.4f}\")\n\n\n\n\n\n\n\n\n\nAccuracy: 0.9578\nPrecision: 0.3333\nRecall: 0.0026\nF1-Score: 0.0051\n\n\n\nAccuracy (95.78%): This is high, indicating that a large proportion of predictions match the actual labels. However, in the context of class imbalance, this metric can be misleading.\nPrecision (33.33%): When the model predicts the positive class, it is correct about one-third of the time. This is relatively low and may be concerning if the positive class is of significant interest.\nRecall (0.26%): This is very low, indicating that the model identifies only a tiny fraction of actual positive cases.\nF1-Score (0.51%): This is extremely low, suggesting that the balance between precision and recall is poor. The model is not effective in predicting positive cases."
  },
  {
    "objectID": "Clustering_Python.html#hierarchical-clustering-1",
    "href": "Clustering_Python.html#hierarchical-clustering-1",
    "title": "Clustering",
    "section": "Hierarchical clustering",
    "text": "Hierarchical clustering\n\n\nCode\n# Hierarchical Clustering\nZ = linkage(data_scaled, 'ward')\n\n# Plot dendrogram\nplt.figure(figsize=(10, 7))\ndendrogram(Z, truncate_mode='lastp', p=12, leaf_rotation=45., leaf_font_size=15., show_contracted=True)\nplt.title('Hierarchical Clustering Dendrogram')\nplt.xlabel('Cluster size')\nplt.ylabel('Distance')\nplt.show()\n\n\n# AGGLOMERATIVE CLUSTERING\nopt_labels=maximize_silhouette(data_scaled,algo=\"ag\",nmax=15, i_plot=True)\nplot(data_scaled,opt_labels)\n\n\n\n\n\n\n\n\n\nOPTIMAL PARAMETER = 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe optimal parameter for DBSCAN method is 2, at parameter = 3.5, the Silhouette score is maximized.\n\nFinal Results for optimal clusters of Hierarchical clustering\n\n\nCode\noptimal_clusters = 2\nagglom_final = AgglomerativeClustering(n_clusters=optimal_clusters, linkage='ward')\nhierarchical_clusters = agglom_final.fit_predict(data_scaled)\n\n# Plot confusion matrix for Hierarchical Clustering\ncm_hierarchical = confusion_matrix(target, hierarchical_clusters)\nfig_hierarchical = plot_confusion_matrix(cm_hierarchical, class_names)\nplt.title('Confusion Matrix for Hierarchical Clustering')\nplt.show()\n\nprecision = precision_score(target, hierarchical_clusters)\nrecall = recall_score(target, hierarchical_clusters)\nf1 = f1_score(target, hierarchical_clusters)\naccuracy = accuracy_score(target, hierarchical_clusters)\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1-Score: {f1:.4f}\")\n\n\n\n\n\n\n\n\n\nAccuracy: 0.4162\nPrecision: 0.0150\nRecall: 0.1990\nF1-Score: 0.0279\n\n\n\nAccuracy (41.62%): This is relatively low, indicating that a significant number of predictions were incorrect.\nPrecision (1.50%): This is very low, suggesting that when the model predicts the positive class, it is correct only 1.5% of the time.\nRecall (19.90%): This is also low, indicating that the model identified less than 20% of all actual positive cases.\nF1-Score (2.79%): This very low score indicates a poor balance between precision and recall, suggesting the model is not effective in predicting the positive class accurately."
  },
  {
    "objectID": "Clustering_Python.html#conclusions",
    "href": "Clustering_Python.html#conclusions",
    "title": "Clustering",
    "section": "Conclusions",
    "text": "Conclusions\n\nAfter evaluating the three clustering models—K-Means, DBSCAN, and Hierarchical Clustering—we can infer several conclusions about their performance to the dataset.\nK-Means showed moderate effectiveness, with a balance between true negatives and positives. However, its precision was low, indicating a significant number of false positives. DBSCAN struggled with this dataset, as shown by low precision and recall, suggesting that its density-based clustering was not suitable for the structure of data or the binary nature of the target variable. Hierarchical Clustering underperformed, with low accuracy and a high rate of false positives, indicating difficulty in distinguishing between the classes.\nEach method has its strength and drawbacks, but none offered a robust solution for the given data. K-Means might be preferred for its relative simplicity and better performance in this specific context.\nFuture directions could involve more sophisticated techniques such as ensemble methods that combine multiple clustering models to improve overall performance."
  },
  {
    "objectID": "Code.html",
    "href": "Code.html",
    "title": "Code",
    "section": "",
    "text": "Please access all codes of this project through Github Links"
  },
  {
    "objectID": "Code.html#code-links",
    "href": "Code.html#code-links",
    "title": "Code",
    "section": "",
    "text": "Please access all codes of this project through Github Links"
  },
  {
    "objectID": "Data.html",
    "href": "Data.html",
    "title": "Codes and Data",
    "section": "",
    "text": "Please click the link below to access codes and data\n\nGithub Repo Link"
  },
  {
    "objectID": "Data.html#link-to-data-and-codes",
    "href": "Data.html#link-to-data-and-codes",
    "title": "Codes and Data",
    "section": "",
    "text": "Please click the link below to access codes and data\n\nGithub Repo Link"
  },
  {
    "objectID": "Data_Gathering.html",
    "href": "Data_Gathering.html",
    "title": "Data Gathering",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(DT)"
  },
  {
    "objectID": "Data_Gathering.html#notes",
    "href": "Data_Gathering.html#notes",
    "title": "Data Gathering",
    "section": "Notes",
    "text": "Notes\nFor data gethering, accessing health data especially in chronic disease data is quite hard by using APIs, the only API data I will be using is the Pima diabetes data generated from R. Also, it is challenging to find Text data for my topic, so all the data in the project will be record data."
  },
  {
    "objectID": "Data_Gathering.html#data-sources-from-online",
    "href": "Data_Gathering.html#data-sources-from-online",
    "title": "Data Gathering",
    "section": "Data Sources from online",
    "text": "Data Sources from online\n\nU.S. Chronic Disease Indicators (CDI)\nCDC’s Division of Population Health provides cross-cutting set of 124 indicators that were developed by consensus and that allows states and territories and large metropolitan areas to uniformly define, collect, and report chronic disease data that are important to public health practice and available for states, territories and large metropolitan areas. In addition to providing access to state-specific indicator data, the CDI web site serves as a gateway to additional information and data resources.\n\n\n\nCode\nus_chronic &lt;- read_csv(\"data/U.S._Chronic_Disease_Indicators__CDI_.csv\")\ndatatable(head(us_chronic))\n\n\n\n\n\n\n\n\nFramingham Heart Study Longitudinal Data\nThe Framingham Heart Study is a long term prospective study of the etiology of cardiovascular disease among a population of free living subjects in the community of Framingham, Massachusetts. The Framingham Heart Study was a landmark study in epidemiology in that it was the first prospective study of cardiovascular disease and identified the concept of risk factors and their joint effects. The study began in 1948 and 5,209 subjects were initially enrolled in the study. Participants have been examined biennially since the inception of the study and all subjects are continuously followed through regular surveillance for cardiovascular outcomes. Clinic examination data has included cardiovascular disease risk factors and markers of disease such as blood pressure, blood chemistry, lung function, smoking history, health behaviors, ECG tracings, Echocardiography, and medication use. Through regular surveillance of area hospitals, participant contact, and death certificates, the Framingham Heart Study reviews and adjudicates events for the occurrence of Angina Pectoris, Myocardial Infarction, Heart Failure, and Cerebrovascular disease.\n\n\nCode\nheart_study &lt;- read_csv(\"data/frmgham2.csv\")\ndatatable(head(heart_study))\n\n\n\n\n\n\n\n\n\n[Diabetes Reddit Data]\nOur project utilizes text datasets from the r/diabetes subreddit, a community of over 134,000 members, spanning the period from June 2023 to July 2024. The dataset includes two primary components: submissions and comments, capturing user-generated content across diverse topics related to diabetes. Submissions represent the original posts made by users, often encompassing detailed narratives, questions, experiences, and informational content, while comments reflect the interactive discussions, advice, and responses generated within the community. This textual data provides a longitudinal snapshot of the discourse in r/diabetes over a 13-month period, enabling the analysis of linguistic patterns, recurring themes, and evolving topics of interest. Natural Language Processing (NLP) techniques are applied to preprocess and analyze this data, including text cleaning, tokenization, and lemmatization, followed by topic modeling and sentiment analysis to uncover the underlying themes and emotional tones in the discussions. The dataset’s temporal coverage allows us to investigate shifts in language usage, sentiment trends, and emerging issues over time, offering a rich foundation for exploring the dynamics of online health-related conversations in the context of diabetes."
  },
  {
    "objectID": "Data_Gathering.html#data-generated-from-r-api",
    "href": "Data_Gathering.html#data-generated-from-r-api",
    "title": "Data Gathering",
    "section": "Data generated from R API",
    "text": "Data generated from R API\n\nPimaIndiansDiabetes\nThe dataset contains test results obtained from a population of women, all of whom were at least 21 years old, of Pima Indian heritage, These data were originally collected by the US National Institute of Diabetes and Digestive and Kidney Diseases and can be accessed directly from the PimaIndiansDiabetes2 dataset. The dataset consists of 768 observations encompassing nine variables. These variables include the number of times of pregency, their plasma glucose concentration , diastolic blood pressure measured in millimeters of mercury (mm Hg), triceps skin fold thickness in millimeters (mm), 2-hour serum insulin levels expressed in micro international units per milliliter (mu U/ml), body mass index (BMI) calculated as weight in kilograms divided by the square of height in meters, the diabetes pedigree function, the age of the individuals in years, and a factor variable indicating the diabetes test result as either negative or positive.\n\n\nCode\nlibrary(mlbench)\ndata(PimaIndiansDiabetes)\ndata &lt;- PimaIndiansDiabetes\ndatatable(data)"
  },
  {
    "objectID": "Dimensionality_reduction_python.html",
    "href": "Dimensionality_reduction_python.html",
    "title": "Dimensionality reduction in Python",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport seaborn as sns"
  },
  {
    "objectID": "Dimensionality_reduction_python.html#introduction",
    "href": "Dimensionality_reduction_python.html#introduction",
    "title": "Dimensionality reduction in Python",
    "section": "Introduction",
    "text": "Introduction\nIn the world of data science and chronic diseases, one of the most challenging thing is dealing with high-dimensional datasets. Dimensionality reduction techniques enables us to extract valuable insights from complex, high dimensional data while reducing computational burdens. In this coding section of our chronic diseases data science project, we employ dimensionality reduction methods that will not only strengthen our analyses but also discern hidden patterns, contribute to the development of effective strategies for disease prevention. By utilizing the power of these techniques, we advance our understanding and ability to address these critical public health concerns. For this part, I will be using the Framingham heart study data set."
  },
  {
    "objectID": "Dimensionality_reduction_python.html#data-preparation",
    "href": "Dimensionality_reduction_python.html#data-preparation",
    "title": "Dimensionality reduction in Python",
    "section": "Data preparation",
    "text": "Data preparation\n\n\nCode\n# Load the Frammingham Heart Study data set\ndata = pd.read_csv(\"data/frmgham2.csv\")\ndata.head()\n\n\n\n\n\n\n\n\n\nRANDID\nSEX\nTOTCHOL\nAGE\nSYSBP\nDIABP\nCURSMOKE\nCIGPDAY\nBMI\nDIABETES\n...\nCVD\nHYPERTEN\nTIMEAP\nTIMEMI\nTIMEMIFC\nTIMECHD\nTIMESTRK\nTIMECVD\nTIMEDTH\nTIMEHYP\n\n\n\n\n0\n2448\n1\n195.0\n39\n106.0\n70.0\n0\n0.0\n26.97\n0\n...\n1\n0\n8766\n6438\n6438\n6438\n8766\n6438\n8766\n8766\n\n\n1\n2448\n1\n209.0\n52\n121.0\n66.0\n0\n0.0\nNaN\n0\n...\n1\n0\n8766\n6438\n6438\n6438\n8766\n6438\n8766\n8766\n\n\n2\n6238\n2\n250.0\n46\n121.0\n81.0\n0\n0.0\n28.73\n0\n...\n0\n0\n8766\n8766\n8766\n8766\n8766\n8766\n8766\n8766\n\n\n3\n6238\n2\n260.0\n52\n105.0\n69.5\n0\n0.0\n29.43\n0\n...\n0\n0\n8766\n8766\n8766\n8766\n8766\n8766\n8766\n8766\n\n\n4\n6238\n2\n237.0\n58\n108.0\n66.0\n0\n0.0\n28.50\n0\n...\n0\n0\n8766\n8766\n8766\n8766\n8766\n8766\n8766\n8766\n\n\n\n\n5 rows × 39 columns\n\n\n\n\nMissing data handelling and normalization\n\n\nCode\n# Check for missing data and calculate percentages\nmissing_data = data.isnull().mean()\n\n# Drop columns with more than 50% missing data\nthreshold = 0.5\ncolumns_to_drop = missing_data[missing_data &gt; threshold].index\ndata.drop(columns=columns_to_drop, inplace=True)\ndata = data.iloc[:, 1:]\n\n# Remove rows with missing data as their percentages are low\ndata.dropna(inplace=True)\n\n# Scale the data\nscaler = StandardScaler()\ndata_scaled = scaler.fit_transform(data)\ndata.head()\n\n\n\n\n\n\n\n\n\nSEX\nTOTCHOL\nAGE\nSYSBP\nDIABP\nCURSMOKE\nCIGPDAY\nBMI\nDIABETES\nBPMEDS\n...\nCVD\nHYPERTEN\nTIMEAP\nTIMEMI\nTIMEMIFC\nTIMECHD\nTIMESTRK\nTIMECVD\nTIMEDTH\nTIMEHYP\n\n\n\n\n0\n1\n195.0\n39\n106.0\n70.0\n0\n0.0\n26.97\n0\n0.0\n...\n1\n0\n8766\n6438\n6438\n6438\n8766\n6438\n8766\n8766\n\n\n2\n2\n250.0\n46\n121.0\n81.0\n0\n0.0\n28.73\n0\n0.0\n...\n0\n0\n8766\n8766\n8766\n8766\n8766\n8766\n8766\n8766\n\n\n3\n2\n260.0\n52\n105.0\n69.5\n0\n0.0\n29.43\n0\n0.0\n...\n0\n0\n8766\n8766\n8766\n8766\n8766\n8766\n8766\n8766\n\n\n4\n2\n237.0\n58\n108.0\n66.0\n0\n0.0\n28.50\n0\n0.0\n...\n0\n0\n8766\n8766\n8766\n8766\n8766\n8766\n8766\n8766\n\n\n5\n1\n245.0\n48\n127.5\n80.0\n1\n20.0\n25.34\n0\n0.0\n...\n0\n0\n8766\n8766\n8766\n8766\n8766\n8766\n8766\n8766\n\n\n\n\n5 rows × 36 columns"
  },
  {
    "objectID": "Dimensionality_reduction_python.html#pca-method",
    "href": "Dimensionality_reduction_python.html#pca-method",
    "title": "Dimensionality reduction in Python",
    "section": "PCA method",
    "text": "PCA method\n\nPCA results plots\n\n\nCode\n# Apply PCA\npca = PCA(n_components=2)\npca_result = pca.fit_transform(data_scaled)\n\n# Plot the PCA results\nsns.scatterplot(x=pca_result[:, 0], y=pca_result[:, 1])\n\nplt.title('PCA Results')\nplt.xlabel('First Principal Component')\nplt.ylabel('Second Principal Component')\n\nplt.figure()\nsns.barplot(x=[f\"PC{i+1}\" for i in range(pca.n_components_)], y=pca.explained_variance_ratio_)\nplt.title('Explained Variance by PCA Components')\nplt.ylabel('Explained Variance Ratio')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPCA variance table for the first two principal component\n\n\nCode\n# Create a DataFrame with the explained variance and cumulative variance\nexplained_variance = pca.explained_variance_ratio_\ncumulative_variance = pca.explained_variance_ratio_.cumsum()\n\npca_table = pd.DataFrame({'Principal Component': [f\"PC{i+1}\" for i in range(len(explained_variance))],\n                          'Explained Variance': explained_variance,\n                          'Cumulative Variance': cumulative_variance})\n\nprint(pca_table)\n\n\n  Principal Component  Explained Variance  Cumulative Variance\n0                 PC1            0.256414             0.256414\n1                 PC2            0.105013             0.361428\n\n\n\n\nPCA Biplot\n\n\nCode\n# Get the loadings\nloadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n\n# Create a new matplotlib figure and axis\nfig, ax = plt.subplots(figsize=(10, 7))\n\n# Plot the loadings for each feature as arrows(I don't know how to draw biplots, used some assitance from Chatgpt)\nfor i, (loading1, loading2) in enumerate(loadings):\n    ax.arrow(0, 0, loading1, loading2, head_width=0.05, head_length=0.1, length_includes_head=True, color='red')\n    plt.text(loading1 * 1.2, loading2 * 1.2, data.columns[i], color='black', ha='center', va='center')\n\n# Set plot labels and title\nax.set_xlabel('First Principal Component')\nax.set_ylabel('Second Principal Component')\nax.set_title('PCA Biplot')\nax.axhline(0, color='grey', lw=1)\nax.axvline(0, color='grey', lw=1)\nax.grid(True)\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nInterpretation\n\nThe PCA analysis results show that the first two principal components (PC1 and PC2) plays a significant portion of the variance in the dataset. PC1 explains 25.6414% of the variance, and when combined with PC2 (10.5013%), they contain 36.1428% of the variance in total.\nThe scree plot and the table reflect a rapid drop in variance explained by each subsequent principal component after the first, which is typical in PCA analysis. This suggests that PC1 captures the most significant variance in the data, but there’s still a meaningful amount of variation represented in PC2.\nDepending on the specific context of data science analysis, considering reducing the dimensionality of the data to these two components for further analysis might be beneficial.\nThe PCA biplot demonstartes that variables such as DIABP (diastolic blood pressure), AGE, HYPERTEN (hypertension), SYSBP (systolic blood pressure), and PREVHYP (previous hypertension) have a pronounced influence on the first principal component. Conversely, variables like CURSMOKE (current smoker status), CIGPDAY (cigarettes per day), and TIMEHYP (time to hypertension development) has a significant impact on the second principal component. This distinction provided by PCA enables optimal feature engineering. The PCA analysis effectively reduces the complexity of the data, allowing for a more manageable interpretation of the key features."
  },
  {
    "objectID": "Dimensionality_reduction_python.html#t-sne-method",
    "href": "Dimensionality_reduction_python.html#t-sne-method",
    "title": "Dimensionality reduction in Python",
    "section": "t-SNE Method",
    "text": "t-SNE Method\n\n\nCode\n# t-SNE analysis with different perplexity values\nps = [5, 30, 50, 100]\n\nfor p in ps:\n    tsne = TSNE(n_components=2, perplexity=p, random_state=42)\n    tsne_result = tsne.fit_transform(data_scaled)\n    tsne_df = pd.DataFrame(tsne_result, columns=['TSNE1', 'TSNE2'])\n\n    sns.scatterplot(data=tsne_df, x='TSNE1', y='TSNE2')\n    plt.title(f't-SNE with Perplexity {p}')\n    plt.xlabel('Component 1')\n    plt.ylabel('Component 2')\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\nWith perplexity 5, the plot shows numerous small clusters, indicating that the model is capturing more local structure in the data.\nPerplexity 30 presents fewer, larger clusters, suggesting a more balanced view that incorporates broader data relationships.\nAt perplexity 50, the clusters are less distinct but still separate, which might mean the model is starting to prioritize the global data structure over local nuances.\nFinally, perplexity 100 leads to even more overlap between clusters, showing that the model is now focusing mainly on the broader patterns in the data."
  },
  {
    "objectID": "Dimensionality_reduction_python.html#conclusions",
    "href": "Dimensionality_reduction_python.html#conclusions",
    "title": "Dimensionality reduction in Python",
    "section": "Conclusions",
    "text": "Conclusions\n\nPCA and t-SNE are both powerful techniques for dimensionality reduction, each with distinct characteristics suitable for different types of data analysis.\nPCA is a linear technique that reduces dimensions by transforming the data into a new coordinate system where the greatest variances by any projection of the data come to lie on the first coordinates, known as principal components. It is proficient at preserving global structure and is computationally efficient, making it suitable for datasets where linear relationships are dominant.\nOn the other hand, t-SNE is a non-linear technique, which is proficient at finding out local structures and clusters within the data. Unlike PCA, t-SNE can capture non-linear relationships by mapping the high-dimensional data to a lower-dimensional space in a way that preserves the data’s local neighborhood structure. This makes t-SNE particularly useful for exploratory data analysis and for datasets where the underlying structure is non-linear."
  },
  {
    "objectID": "Naïve_Bayes_python.html",
    "href": "Naïve_Bayes_python.html",
    "title": "Naïve Bayes in Python",
    "section": "",
    "text": "Naive Bayes classification, grounded in Bayes’ theorem, is a supervised machine learning algorithm designed to categorize items into pre-established labels, it is famous for its proficiency in text-analysis.\nThe Naive Bayes classification use the Bayes theorem as foundation, the Bayes theorem is given by the conditional probability of some hypothesis given some evidence, the function of the Bayes theorem is given by : \\(P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\\).\nAccording to the name “Naive Bayes Classifier”, the object of the supervised machine learning method is having the ability to classify or predict object’s label based on the given labels. The algorithm aims to calculate the probability of each class given a set of input features and then assigns the class with the highest probability as the predicted class for the input.\n\n\n\nThe Gaussian Naive Bayes is used when assume our label/feature data has a normal distribution, it is best suitable for continuous data with a normal distribution.\nThe multinational Naive Bayes is implemented if the data has a discrete variable, it is best suitable for text classification.\nBernoulli Naive Bayes is used on binary data, it is best suitable for document classification where it is either present or absence."
  },
  {
    "objectID": "Naïve_Bayes_python.html#introduction-to-naïve-bayes",
    "href": "Naïve_Bayes_python.html#introduction-to-naïve-bayes",
    "title": "Naïve Bayes in Python",
    "section": "",
    "text": "Naive Bayes classification, grounded in Bayes’ theorem, is a supervised machine learning algorithm designed to categorize items into pre-established labels, it is famous for its proficiency in text-analysis.\nThe Naive Bayes classification use the Bayes theorem as foundation, the Bayes theorem is given by the conditional probability of some hypothesis given some evidence, the function of the Bayes theorem is given by : \\(P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\\).\nAccording to the name “Naive Bayes Classifier”, the object of the supervised machine learning method is having the ability to classify or predict object’s label based on the given labels. The algorithm aims to calculate the probability of each class given a set of input features and then assigns the class with the highest probability as the predicted class for the input.\n\n\n\nThe Gaussian Naive Bayes is used when assume our label/feature data has a normal distribution, it is best suitable for continuous data with a normal distribution.\nThe multinational Naive Bayes is implemented if the data has a discrete variable, it is best suitable for text classification.\nBernoulli Naive Bayes is used on binary data, it is best suitable for document classification where it is either present or absence."
  },
  {
    "objectID": "Naïve_Bayes_python.html#data-preparation-for-naïve-bayes",
    "href": "Naïve_Bayes_python.html#data-preparation-for-naïve-bayes",
    "title": "Naïve Bayes in Python",
    "section": "Data preparation for Naïve Bayes",
    "text": "Data preparation for Naïve Bayes\n\nLoad the cleaned data of the Framingham Heart Study Longitudinal Data,\n\nFor this part of analysis, I will be using cardiovascular diseases (CVD) as my target variable.\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_selection import f_classif, SelectKBest\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# Load the Frammingham Heart Study data set\ndata = pd.read_csv(\"data/frmgham2.csv\")\ndata.head()\ndata.describe\n\n\n&lt;bound method NDFrame.describe of         RANDID  SEX  TOTCHOL  AGE  SYSBP  DIABP  CURSMOKE  CIGPDAY    BMI  \\\n0         2448    1    195.0   39  106.0   70.0         0      0.0  26.97   \n1         2448    1    209.0   52  121.0   66.0         0      0.0    NaN   \n2         6238    2    250.0   46  121.0   81.0         0      0.0  28.73   \n3         6238    2    260.0   52  105.0   69.5         0      0.0  29.43   \n4         6238    2    237.0   58  108.0   66.0         0      0.0  28.50   \n...        ...  ...      ...  ...    ...    ...       ...      ...    ...   \n11622  9998212    1    173.0   46  126.0   82.0         0      0.0  19.17   \n11623  9998212    1    153.0   52  143.0   89.0         0      0.0  25.74   \n11624  9999312    2    196.0   39  133.0   86.0         1     30.0  20.91   \n11625  9999312    2    240.0   46  138.0   79.0         1     20.0  26.39   \n11626  9999312    2      NaN   50  147.0   96.0         1     10.0  24.19   \n\n       DIABETES  ...  CVD  HYPERTEN  TIMEAP  TIMEMI  TIMEMIFC  TIMECHD  \\\n0             0  ...    1         0    8766    6438      6438     6438   \n1             0  ...    1         0    8766    6438      6438     6438   \n2             0  ...    0         0    8766    8766      8766     8766   \n3             0  ...    0         0    8766    8766      8766     8766   \n4             0  ...    0         0    8766    8766      8766     8766   \n...         ...  ...  ...       ...     ...     ...       ...      ...   \n11622         0  ...    0         1    8766    8766      8766     8766   \n11623         0  ...    0         1    8766    8766      8766     8766   \n11624         0  ...    0         1    8766    8766      8766     8766   \n11625         0  ...    0         1    8766    8766      8766     8766   \n11626         0  ...    0         1    8766    8766      8766     8766   \n\n       TIMESTRK  TIMECVD  TIMEDTH  TIMEHYP  \n0          8766     6438     8766     8766  \n1          8766     6438     8766     8766  \n2          8766     8766     8766     8766  \n3          8766     8766     8766     8766  \n4          8766     8766     8766     8766  \n...         ...      ...      ...      ...  \n11622      8766     8766     8766        0  \n11623      8766     8766     8766        0  \n11624      8766     8766     8766     4201  \n11625      8766     8766     8766     4201  \n11626      8766     8766     8766     4201  \n\n[11627 rows x 39 columns]&gt;\n\n\n\n\nMissing data handeling\n\n\nCode\n# missing data handeling\nprint(data.isna().sum())\n#data.dropna()\n\n\nRANDID         0\nSEX            0\nTOTCHOL      409\nAGE            0\nSYSBP          0\nDIABP          0\nCURSMOKE       0\nCIGPDAY       79\nBMI           52\nDIABETES       0\nBPMEDS       593\nHEARTRTE       6\nGLUCOSE     1440\neduc         295\nPREVCHD        0\nPREVAP         0\nPREVMI         0\nPREVSTRK       0\nPREVHYP        0\nTIME           0\nPERIOD         0\nHDLC        8600\nLDLC        8601\nDEATH          0\nANGINA         0\nHOSPMI         0\nMI_FCHD        0\nANYCHD         0\nSTROKE         0\nCVD            0\nHYPERTEN       0\nTIMEAP         0\nTIMEMI         0\nTIMEMIFC       0\nTIMECHD        0\nTIMESTRK       0\nTIMECVD        0\nTIMEDTH        0\nTIMEHYP        0\ndtype: int64\n\n\n\nRemoving data variables that has 30% or more of missing data\n\n\n\nCode\n# Calculate the percentage of missing data in each column\nmissing_percentage = (data.isna().sum() / len(data)) * 100\n\n# Set the threshold for missing data (30%)\nthreshold = 30\n\n# Identify columns with missing data exceeding the threshold\ncolumns_to_remove = missing_percentage[missing_percentage &gt; threshold].index\nprint(columns_to_remove)\n\n# Remove the identified columns from the DataFrame\ndata = data.drop(columns=columns_to_remove)\ndata.dropna(inplace=True)\n\ndata.head()\n\n\nIndex(['HDLC', 'LDLC'], dtype='object')\n\n\n\n\n\n\n\n\n\nRANDID\nSEX\nTOTCHOL\nAGE\nSYSBP\nDIABP\nCURSMOKE\nCIGPDAY\nBMI\nDIABETES\n...\nCVD\nHYPERTEN\nTIMEAP\nTIMEMI\nTIMEMIFC\nTIMECHD\nTIMESTRK\nTIMECVD\nTIMEDTH\nTIMEHYP\n\n\n\n\n0\n2448\n1\n195.0\n39\n106.0\n70.0\n0\n0.0\n26.97\n0\n...\n1\n0\n8766\n6438\n6438\n6438\n8766\n6438\n8766\n8766\n\n\n2\n6238\n2\n250.0\n46\n121.0\n81.0\n0\n0.0\n28.73\n0\n...\n0\n0\n8766\n8766\n8766\n8766\n8766\n8766\n8766\n8766\n\n\n3\n6238\n2\n260.0\n52\n105.0\n69.5\n0\n0.0\n29.43\n0\n...\n0\n0\n8766\n8766\n8766\n8766\n8766\n8766\n8766\n8766\n\n\n4\n6238\n2\n237.0\n58\n108.0\n66.0\n0\n0.0\n28.50\n0\n...\n0\n0\n8766\n8766\n8766\n8766\n8766\n8766\n8766\n8766\n\n\n5\n9428\n1\n245.0\n48\n127.5\n80.0\n1\n20.0\n25.34\n0\n...\n0\n0\n8766\n8766\n8766\n8766\n8766\n8766\n8766\n8766\n\n\n\n\n5 rows × 37 columns\n\n\n\n\n\nSplit data into train/test/validation sets\n\n\nCode\n# Split target variables and predictor variables\nX = data.drop('CVD', axis=1)\ny = data['CVD']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42) \n\nX_train.head()\n\n\n\n\n\n\n\n\n\nRANDID\nSEX\nTOTCHOL\nAGE\nSYSBP\nDIABP\nCURSMOKE\nCIGPDAY\nBMI\nDIABETES\n...\nSTROKE\nHYPERTEN\nTIMEAP\nTIMEMI\nTIMEMIFC\nTIMECHD\nTIMESTRK\nTIMECVD\nTIMEDTH\nTIMEHYP\n\n\n\n\n1593\n1333048\n2\n241.0\n71\n193.0\n106.0\n1\n20.0\n18.63\n0\n...\n1\n1\n8766\n8766\n8766\n8766\n5378\n5378\n8766\n0\n\n\n66\n55965\n2\n256.0\n66\n126.0\n82.0\n0\n0.0\n25.43\n0\n...\n0\n1\n8297\n8766\n8766\n8297\n8766\n8766\n8766\n7951\n\n\n1581\n1315580\n1\n259.0\n64\n121.0\n65.0\n1\n20.0\n21.19\n0\n...\n0\n1\n8766\n8766\n8766\n8766\n8766\n8766\n8766\n8178\n\n\n11247\n9655204\n2\n238.0\n51\n123.0\n80.0\n0\n0.0\n22.19\n0\n...\n0\n1\n8766\n8766\n8766\n8766\n8766\n8766\n8766\n0\n\n\n8354\n7206124\n2\n287.0\n69\n209.0\n98.0\n0\n0.0\n26.57\n0\n...\n0\n1\n8766\n8766\n8766\n8766\n8766\n8766\n8766\n0\n\n\n\n\n5 rows × 36 columns\n\n\n\nThe objective for spliting data into train/test/validation sets is to prevent model from overfitting and improve generalization of the model."
  },
  {
    "objectID": "Naïve_Bayes_python.html#feature-selection-using-anova-f-test",
    "href": "Naïve_Bayes_python.html#feature-selection-using-anova-f-test",
    "title": "Naïve Bayes in Python",
    "section": "Feature selection using ANOVA F-test",
    "text": "Feature selection using ANOVA F-test\n\n\nCode\ns= SelectKBest(score_func=f_classif, k=25)  \nX_new = s.fit_transform(X, y)\nsf = X.columns[s.get_support()]\nprint(\"Selected features:\", sf)\n\n\nSelected features: Index(['SEX', 'AGE', 'SYSBP', 'DIABP', 'DIABETES', 'PREVCHD', 'PREVAP',\n       'PREVMI', 'PREVSTRK', 'PREVHYP', 'DEATH', 'ANGINA', 'HOSPMI', 'MI_FCHD',\n       'ANYCHD', 'STROKE', 'HYPERTEN', 'TIMEAP', 'TIMEMI', 'TIMEMIFC',\n       'TIMECHD', 'TIMESTRK', 'TIMECVD', 'TIMEDTH', 'TIMEHYP'],\n      dtype='object')"
  },
  {
    "objectID": "Naïve_Bayes_python.html#model-training",
    "href": "Naïve_Bayes_python.html#model-training",
    "title": "Naïve Bayes in Python",
    "section": "Model training",
    "text": "Model training\n\nTraining sets results\n\n\nCode\n# Train the Naive Bayes model using selected features\nnb_model = GaussianNB()\nnb_model.fit(X_train[sf], y_train)\n\n# Obtain evaluation metrics for both training sets/validation sets\ny_train_pred = nb_model.predict(X_train[sf])\ny_val_pred = nb_model.predict(X_val[sf])\n\nval_accuracy = accuracy_score(y_val, y_val_pred)\ntrain_accuracy = accuracy_score( y_train, y_train_pred)\n\nprint(f\"Accuracy for training set: {train_accuracy}\")\nprint(classification_report( y_train, y_train_pred))\n\n\nAccuracy for training set: 0.899391335481561\n              precision    recall  f1-score   support\n\n           0       0.96      0.90      0.93      4188\n           1       0.75      0.89      0.82      1398\n\n    accuracy                           0.90      5586\n   macro avg       0.86      0.90      0.87      5586\nweighted avg       0.91      0.90      0.90      5586\n\n\n\n\n\nValidation sets results\n\n\nCode\nprint(f\"Accuracy for validation set: {val_accuracy}\")\nprint(classification_report(y_val, y_val_pred))\n\n# Confusion Matrix for Validation Data\ncm_val = confusion_matrix(y_val, y_val_pred)\nprint(cm_val)\n\n\nAccuracy for validation set: 0.916219119226638\n              precision    recall  f1-score   support\n\n           0       0.97      0.92      0.94      1385\n           1       0.79      0.91      0.85       477\n\n    accuracy                           0.92      1862\n   macro avg       0.88      0.91      0.89      1862\nweighted avg       0.92      0.92      0.92      1862\n\n[[1272  113]\n [  43  434]]\n\n\n\nInterpretation :\n\nThe Naive Bayes classifier works really well when tested, showing high levels of correct predictions and a good balance in its ability to identify true cases of the condition it’s predicting. The F1-scores, which combine precision and recall, are also high, indicating the model is consistent in its predictions. Moreover, the model is slightly more accurate on the vaildation data than on the data it was trained on, which suggests its generalization when used in real-world data. Also, the high accuracy score on validation sets also indicates that the model neither over-fitting nor under-fitting, but reaching a balance between the two. Overall, these results highlight the model as a dependable tool for predictive modelling."
  },
  {
    "objectID": "Naïve_Bayes_python.html#visualizations",
    "href": "Naïve_Bayes_python.html#visualizations",
    "title": "Naïve Bayes in Python",
    "section": "Visualizations",
    "text": "Visualizations\n\nConfusion matrix heatmap\n\n\nCode\n# Confusion Matrix Heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm_val, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted Negative', 'Predicted Positive'], yticklabels=['Actual Negative', 'Actual Positive'])\nplt.title('Confusion Matrix for Validation Data')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe confusion matrix heatmap is a visual interpretation of the fitted model.\n\n\nAccuracy evaluation bar plots\n\n\nCode\n# Metrics for the training set\nta = accuracy_score(y_train, y_train_pred)\ntp = precision_score(y_train, y_train_pred)\ntr = recall_score(y_train, y_train_pred)\ntf1 = f1_score(y_train, y_train_pred)\n\n# Metrics for the validation set\nva = accuracy_score(y_val, y_val_pred)\nvp = precision_score(y_val, y_val_pred)\nvr = recall_score(y_val, y_val_pred)\nvf1 = f1_score(y_val, y_val_pred)\n\n# Bar plot of accuracy metrics\nmetrics_df = pd.DataFrame({\n    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n    'Training': [ta, tp, tr, tf1],\n    'Validation': [va, vp, vr, vf1]\n})\n\nmetrics_df.set_index('Metric', inplace=True)\nmetrics_df.plot.bar(rot=0, figsize=(10, 6))\nplt.title('Comparison of Training and Validation Metrics')\nplt.ylabel('Score')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe bar plot demonstrates comparison of scores of Accuracy, precisoin, recall, and F-1 score between training and validation models, with higher scores on every metrics of the validation models, suggesting that the model learns well on the training data and also performs strongly on unseen data, suggesting a good model performance.\n\n\nROC curve\n\n\nCode\n# Predict probabilities for the validation set\nyp = nb_model.predict_proba(X_val[sf])[:, 1]\n\n# Compute ROC curve and ROC area for the validation set\nfpr, tpr, _ = roc_curve(y_val, yp)\nroc = auc(fpr, tpr)\n\n# Plot ROC Curve for the validation set\nplt.figure(figsize=(7, 5))\nplt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve for Validation Set')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n\n\n\n\n\n\n\nA receiver operating characteristic curve, or ROC curve, is a graphical plot that demonstrates the performance of a binary classifier model, it is the plot of the true positive rate (TPR) against the false positive rate (FPR) at each threshold setting. According to ROC curve, the area = 0.97 sugggesting a good overall performance, where the area = 1 indicating a perfect predictive model. The curve indicates a high true positive rate and a low false positive rate, reinforcing the interpretation that the model performs well."
  },
  {
    "objectID": "Naïve_Bayes_python.html#conclusion",
    "href": "Naïve_Bayes_python.html#conclusion",
    "title": "Naïve Bayes in Python",
    "section": "Conclusion",
    "text": "Conclusion\n\nCombining all the insights from the beginning of the analysis, we can conclude that the Naive Bayes model has demonstrated strong predictive performance on cardiovascular diseases (CVD) classifcation.\nThe model’s accuracy levels on both the training and validation sets were pretty high, with the validation accuracy slightly higher that of the training set, which indicates that the model’s ability to generalize well to unseen data. Precision, recall, and F1 scores across both sets were largely balanced, suggesting that the model is reliable in its classification ability.\nAnalysis by the ROC curve for the validation set proves this conclusion, with an AUC of 0.97 reflecting the model’s strong capability in distinguishing between different classes. The proximity of the curve to the upper left corner of the plot reinforces the model’s effectiveness in achieving a high accuracy.\nOverall, these results demonstrates that a Naive Bayes classifier that not only performs well statistically but also holds significant promise for real-world applications. Its high degree of accuracy and its strong balance of precision and recall suggest it could be a valuable tool in medical diagnostics. The consistency in the model’s performance across training and validation datasets indicates its robustness and suggests it would likely perform well in real-world situations, making it a trustworthy tool for predicting the presence of cardiovascular disease."
  }
]